{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionaries for SUMup and SNOWPACK\n",
    "\n",
    "#### Author: Megan Thompson-Munson\n",
    "#### Date created: 20 September 2021\n",
    "\n",
    "This script reads in SUMup observation data and SNOWPACK output, reformats the data, and creates dictionaries that are saved as pickle files.\n",
    "\n",
    "TO DO:\n",
    "- Add CFM capability\n",
    "- Add ```.smet``` capability\n",
    "- Add more comments about input/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN\n",
    "\n",
    "# Select ice sheet\n",
    "icesheet = 'GrIS'\n",
    "\n",
    "# Give path of SNOWPACK output data\n",
    "path = '/projects/metm9666/snowpack/Scripts/Spinup/output/'\n",
    "\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in SUMup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Citation</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Midpoint</th>\n",
       "      <th>StartDepth</th>\n",
       "      <th>StopDepth</th>\n",
       "      <th>Thickness</th>\n",
       "      <th>Density</th>\n",
       "      <th>CoreID</th>\n",
       "      <th>CoreIdx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Core</th>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">6</th>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>73.647102</td>\n",
       "      <td>-38.67720</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>73.647102</td>\n",
       "      <td>-38.67720</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>73.647102</td>\n",
       "      <td>-38.67720</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>73.647102</td>\n",
       "      <td>-38.67720</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>73.647102</td>\n",
       "      <td>-38.67720</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1319</th>\n",
       "      <th>5</th>\n",
       "      <td>187.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.579781</td>\n",
       "      <td>-38.45863</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.03</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>1319</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>187.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.579781</td>\n",
       "      <td>-38.45863</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.03</td>\n",
       "      <td>297.699982</td>\n",
       "      <td>1319</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>187.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.579781</td>\n",
       "      <td>-38.45863</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.03</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>1319</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>187.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.579781</td>\n",
       "      <td>-38.45863</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.03</td>\n",
       "      <td>305.599976</td>\n",
       "      <td>1319</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>187.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.579781</td>\n",
       "      <td>-38.45863</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.03</td>\n",
       "      <td>326.299988</td>\n",
       "      <td>1319</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1652433 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Citation  Timestamp   Latitude  Longitude  Elevation  Midpoint  \\\n",
       "Core Index                                                                   \n",
       "6    0          11.0 2011-05-12  73.647102  -38.67720     3108.0     0.050   \n",
       "     1          11.0 2011-05-12  73.647102  -38.67720     3108.0     0.150   \n",
       "     2          11.0 2011-05-12  73.647102  -38.67720     3108.0     0.250   \n",
       "     3          11.0 2011-05-12  73.647102  -38.67720     3108.0     0.350   \n",
       "     4          11.0 2011-05-12  73.647102  -38.67720     3108.0     0.450   \n",
       "...              ...        ...        ...        ...        ...       ...   \n",
       "1319 5         187.0 2013-06-04  72.579781  -38.45863     3210.0     0.765   \n",
       "     6         187.0 2013-06-04  72.579781  -38.45863     3210.0     0.795   \n",
       "     7         187.0 2013-06-04  72.579781  -38.45863     3210.0     0.825   \n",
       "     8         187.0 2013-06-04  72.579781  -38.45863     3210.0     0.855   \n",
       "     9         187.0 2013-06-04  72.579781  -38.45863     3210.0     0.885   \n",
       "\n",
       "            StartDepth  StopDepth  Thickness     Density  CoreID  CoreIdx  \n",
       "Core Index                                                                 \n",
       "6    0            0.00       0.10       0.10  321.000000       6        0  \n",
       "     1            0.10       0.20       0.10  351.000000       6        1  \n",
       "     2            0.20       0.30       0.10  363.000000       6        2  \n",
       "     3            0.30       0.40       0.10  266.000000       6        3  \n",
       "     4            0.40       0.50       0.10  281.000000       6        4  \n",
       "...                ...        ...        ...         ...     ...      ...  \n",
       "1319 5            0.75       0.78       0.03  274.000000    1319        5  \n",
       "     6            0.78       0.81       0.03  297.699982    1319        6  \n",
       "     7            0.81       0.84       0.03  308.000000    1319        7  \n",
       "     8            0.84       0.87       0.03  305.599976    1319        8  \n",
       "     9            0.87       0.90       0.03  326.299988    1319        9  \n",
       "\n",
       "[1652433 rows x 12 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open latest SUMup dataset\n",
    "sumup = xr.open_dataset('sumup_density_2020_v060121.nc')\n",
    "\n",
    "# Extract data and remove no data\n",
    "su_elev = sumup['Elevation'].values\n",
    "su_lat = sumup['Latitude'].values\n",
    "\n",
    "# Ignore any sea ice data\n",
    "condition = (su_elev>0) & (su_lat>-91)\n",
    "\n",
    "# Exctract data based on condition\n",
    "su_lon = sumup['Longitude'].values[condition]\n",
    "su_depth0 = sumup['Start_Depth'].values[condition]\n",
    "su_depth1 = sumup['Stop_Depth'].values[condition]\n",
    "su_midpoint = sumup['Midpoint'].values[condition]\n",
    "su_density = sumup['Density'].values[condition]\n",
    "su_citation = sumup['Citation'].values[condition]\n",
    "su_date = sumup['Date'].values[condition]\n",
    "su_method = sumup['Method'].values[condition]\n",
    "su_elev = su_elev[condition]\n",
    "su_lat = su_lat[condition]\n",
    "\n",
    "# Create new array of midpoints greater than -9999 (note that last entry in the dataset has no depth)\n",
    "a = su_midpoint\n",
    "b = (su_depth0+su_depth1)/2\n",
    "M = []\n",
    "for i in range(len(a)):\n",
    "    if a[i] < b[i]:\n",
    "        m = b[i]\n",
    "    if a[i] >= b[i]:\n",
    "        m = a[i]\n",
    "    M.append(m)\n",
    "\n",
    "# Some dates are just year (e.g., 'YYYY0000') so this will create a new column with Jan 1 of the year as the date\n",
    "su_timestamp = []\n",
    "for i in range(len(su_date)):\n",
    "    d = su_date[i]\n",
    "    date_str = str(d)\n",
    "    \n",
    "    # These particular dates appear to be very incorrect\n",
    "    if date_str == '19999000.0':\n",
    "        date_str = '19990000.0'\n",
    "    if date_str == '20089620.0':\n",
    "        date_str = '20080620.0'\n",
    "    \n",
    "    year = date_str[0:4]\n",
    "    month = date_str[4:6]\n",
    "    day = date_str[6:8]\n",
    "    \n",
    "    # Add Jan 1 to year-only dates, and change any with 32 days to 31 days\n",
    "    if month == '00':\n",
    "        month = '01'\n",
    "    if day == '00':\n",
    "        day = '01'\n",
    "    if day == '32':\n",
    "        day = '31'\n",
    "    \n",
    "    d = float(year+month+day)\n",
    "    su_timestamp.append(d)\n",
    "\n",
    "su_timestamp = np.array(su_timestamp)\n",
    "    \n",
    "# Create SUMup dataframe\n",
    "su_data = {'Citation':su_citation,'Timestamp':su_timestamp,'Latitude':su_lat,'Longitude':su_lon,\n",
    "              'Elevation':su_elev,'Midpoint':M,'StartDepth':su_depth0,'StopDepth':su_depth1,\n",
    "              'Thickness':su_depth1-su_depth0,'Density':su_density*1000}\n",
    "\n",
    "df = pd.DataFrame(data=su_data)\n",
    "\n",
    "# Turn date into timestamp\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%Y%m%d')\n",
    "\n",
    "# Create a unique index for each core\n",
    "n = -1\n",
    "id0 = []\n",
    "for i in range(len(su_citation)-1):\n",
    "    if (su_citation[i]==su_citation[i-1] and su_lat[i]==su_lat[i-1] and su_lon[i]==su_lon[i-1] and su_method[i]==su_method[i-1]):\n",
    "        index = n\n",
    "    else:\n",
    "        n += 1\n",
    "        index = n\n",
    "    id0.append(index)\n",
    "id0.append(id0[-1])\n",
    "\n",
    "# Give each datapoint within a core index its own index\n",
    "m = -1\n",
    "id1 = []\n",
    "for i in range(len(id0)-1):\n",
    "    if id0[i] == id0[i-1]:\n",
    "        m += 1\n",
    "    else:\n",
    "        m = 0\n",
    "    id1.append(m)\n",
    "id1.append(id1[-1]+1)\n",
    "\n",
    "# Set indices in dataframe\n",
    "df['CoreID'] = id0\n",
    "df['CoreIdx'] = id1\n",
    "idx0 = pd.Series(data=id0,name='Core')\n",
    "idx1 = pd.Series(data=id1,name='Index')\n",
    "idx_arrays = [idx0,idx1]\n",
    "df.index = idx_arrays\n",
    "\n",
    "if icesheet == 'GrIS':\n",
    "    df = df[df.Latitude>0]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata about both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unqiue lat/lon from SUMup and save as new dataframe\n",
    "df_sumuplocs = df[df.CoreIdx==0][['CoreID','Citation','Timestamp','Latitude','Longitude']]\n",
    "df_sumuplocs = df_sumuplocs.reset_index(drop=True)\n",
    "\n",
    "# Read in MERRA-2 location data\n",
    "df_merra2locs = pd.read_table('GrIS_full_station_list.lst',skiprows=1,delim_whitespace=True,usecols=[0,3,4],names=['Station','Latitude','Longitude'])\n",
    "\n",
    "# Extract VIRs\n",
    "VIRs = []\n",
    "for i in range(len(df_merra2locs)):\n",
    "    VIR = df_merra2locs.Station[i][8:]\n",
    "    VIRs.append(VIR)\n",
    "df_merra2locs['VIR'] = VIRs\n",
    "df_merra2locs.drop(columns=['Station'])\n",
    "\n",
    "# Haversine formula for calculating distance between two points on Earth\n",
    "def haversine(lat1,lon1,lat2,lon2):\n",
    "    phi1 = np.deg2rad(lat1)\n",
    "    phi2 = np.deg2rad(lat2)\n",
    "    theta1 = np.deg2rad(lon1)\n",
    "    theta2 = np.deg2rad(lon2)\n",
    "    del_phi = phi2-phi1\n",
    "    del_theta = theta2-theta1\n",
    "    a = np.sin(del_phi/2)**2 + (np.cos(phi1)*np.cos(phi2)*np.sin(del_theta/2)**2)\n",
    "    c = 2*np.arctan2(np.sqrt(a),np.sqrt(1-a))\n",
    "    d = (6371e3)*c # Earth's radius in meters\n",
    "    return d # Meters\n",
    "\n",
    "# Function for finding closest MERRA-2 location to given SUMup location\n",
    "def closest_location(sumuplat,sumuplon):\n",
    "    distance = []\n",
    "    for i in range(len(df_merra2locs)):\n",
    "        lat1 = sumuplat\n",
    "        lon1 = sumuplon\n",
    "        lat2 = df_merra2locs.Latitude[i]\n",
    "        lon2 = df_merra2locs.Longitude[i]\n",
    "        d = haversine(lat1,lon1,lat2,lon2)\n",
    "        distance.append(d)\n",
    "    p = np.where(distance == min(distance))\n",
    "    return df_merra2locs.loc[p]\n",
    "\n",
    "metadata = np.zeros((len(df_sumuplocs),7))\n",
    "for i in range(len(df_sumuplocs)):\n",
    "    metadata[i,0] = df_sumuplocs.CoreID[i]\n",
    "    metadata[i,1] = np.array(df_sumuplocs.Timestamp)[i]\n",
    "    metadata[i,2] = df_sumuplocs.Latitude[i]\n",
    "    metadata[i,3] = df_sumuplocs.Longitude[i]\n",
    "    merra2 = closest_location(df_sumuplocs.Latitude[i],df_sumuplocs.Longitude[i])\n",
    "    metadata[i,4] = merra2.VIR.values[0]\n",
    "    metadata[i,5] = merra2.Latitude.values[0]\n",
    "    metadata[i,6] = merra2.Longitude.values[0]\n",
    "\n",
    "# Create dataframe of metadata and turn float date back into timestamp\n",
    "df_meta = pd.DataFrame(metadata,columns=['CoreID','Timestamp','SUMupLat','SUMupLon',\n",
    "                                         'VIR','MERRALat','MERRALon'])\n",
    "df_meta['Timestamp'] = pd.to_datetime(df_meta['Timestamp'])\n",
    "\n",
    "# Ignore anny missing files\n",
    "df_meta = df_meta[(df_meta.VIR!=107)]\n",
    "df_meta.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CoreID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>SUMupLat</th>\n",
       "      <th>SUMupLon</th>\n",
       "      <th>VIR</th>\n",
       "      <th>MERRALat</th>\n",
       "      <th>MERRALon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>73.647102</td>\n",
       "      <td>-38.677200</td>\n",
       "      <td>494.0</td>\n",
       "      <td>73.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2011-05-24</td>\n",
       "      <td>74.507965</td>\n",
       "      <td>-41.339031</td>\n",
       "      <td>574.0</td>\n",
       "      <td>74.5</td>\n",
       "      <td>-41.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>75.129303</td>\n",
       "      <td>-40.541832</td>\n",
       "      <td>627.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-40.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2010-05-20</td>\n",
       "      <td>75.408035</td>\n",
       "      <td>-41.068619</td>\n",
       "      <td>678.0</td>\n",
       "      <td>75.5</td>\n",
       "      <td>-41.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2010-05-20</td>\n",
       "      <td>75.949409</td>\n",
       "      <td>-42.160610</td>\n",
       "      <td>730.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-41.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>1315.0</td>\n",
       "      <td>2008-06-08</td>\n",
       "      <td>72.549934</td>\n",
       "      <td>-38.309067</td>\n",
       "      <td>417.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>1316.0</td>\n",
       "      <td>2008-06-16</td>\n",
       "      <td>72.579552</td>\n",
       "      <td>-38.505466</td>\n",
       "      <td>416.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>1317.0</td>\n",
       "      <td>2008-06-20</td>\n",
       "      <td>72.549934</td>\n",
       "      <td>-38.309067</td>\n",
       "      <td>417.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>1318.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.635132</td>\n",
       "      <td>-38.514919</td>\n",
       "      <td>416.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>1319.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.579781</td>\n",
       "      <td>-38.458630</td>\n",
       "      <td>416.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>426 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CoreID  Timestamp   SUMupLat   SUMupLon    VIR  MERRALat  MERRALon\n",
       "0       6.0 2011-05-12  73.647102 -38.677200  494.0      73.5   -38.750\n",
       "1       7.0 2011-05-24  74.507965 -41.339031  574.0      74.5   -41.250\n",
       "2       8.0 2011-05-12  75.129303 -40.541832  627.0      75.0   -40.625\n",
       "3       9.0 2010-05-20  75.408035 -41.068619  678.0      75.5   -41.250\n",
       "4      10.0 2010-05-20  75.949409 -42.160610  730.0      76.0   -41.875\n",
       "..      ...        ...        ...        ...    ...       ...       ...\n",
       "421  1315.0 2008-06-08  72.549934 -38.309067  417.0      72.5   -38.125\n",
       "422  1316.0 2008-06-16  72.579552 -38.505466  416.0      72.5   -38.750\n",
       "423  1317.0 2008-06-20  72.549934 -38.309067  417.0      72.5   -38.125\n",
       "424  1318.0 2013-06-04  72.635132 -38.514919  416.0      72.5   -38.750\n",
       "425  1319.0 2013-06-04  72.579781 -38.458630  416.0      72.5   -38.750\n",
       "\n",
       "[426 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in SNOWPACK data and create dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "\n",
    "for i in range(len(df_meta)):\n",
    "    \n",
    "    # Select line of metadata\n",
    "    meta = df_meta.loc[i]\n",
    "    \n",
    "    if icesheet == 'GrIS':\n",
    "        file = path+'VIR'+str(int(meta.VIR))+'_GrIS.pro'\n",
    "        \n",
    "    # Open *.pro file and read in header (44 lines in length)\n",
    "    f = open(file,'r')\n",
    "    for j in range(44):\n",
    "        header = f.readline()\n",
    "        if j == 1:\n",
    "            VIR = int(header[29:-1])\n",
    "        if j == 2:\n",
    "            latitude_sp = float(header[10:-1])\n",
    "        if j == 3:\n",
    "            longitude_sp = float(header[11:-1])\n",
    "        if j == 4:\n",
    "            elevation_sp = float(header[9:-1])\n",
    "    \n",
    "    timestamps_sp = [] # Empty list for storing SNOWPACK timestamps\n",
    "\n",
    "    # Read data line by line\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        linecode = line[0:4] # SNOWPACK gives each data type a 4-digit code\n",
    "\n",
    "        # Extract timestamps and save in a list\n",
    "        if linecode == '0500':\n",
    "            raw_date_sp = line[5:24]\n",
    "            date_sp = datetime.strptime(raw_date_sp,'%d.%m.%Y %H:%M:%S')\n",
    "            timestamp_sp = pd.to_datetime(date_sp)\n",
    "            timestamps_sp.append(timestamp_sp)\n",
    "\n",
    "    # Find SNOWPACK timestamp that's closest to the desired SUMup one\n",
    "    closest = min(timestamps_sp, key=lambda sub: abs(sub - meta.Timestamp))\n",
    "    k = np.where(np.array(timestamps_sp)==closest)[0][0]\n",
    "\n",
    "    # Read data and extract lines corresponding to closest timestamp\n",
    "    for line in data:\n",
    "        linecode = line[0:4] # SNOWPACK gives each data type a 4-digit code\n",
    "\n",
    "        if linecode == '0500':\n",
    "            raw_date_sp = line[5:24]\n",
    "            date_sp = datetime.strptime(raw_date_sp,'%d.%m.%Y %H:%M:%S')\n",
    "            timestamp_sp = pd.to_datetime(date_sp)\n",
    "\n",
    "            if timestamp_sp == closest:\n",
    "\n",
    "                index = k*27 # Each timestamp has 27 elements, so this allows us to get to the start of each new timestamp\n",
    "\n",
    "                # Extract variables of interest by spliting the lines and creating lists of the data\n",
    "                height = list(map(float,data[index+1][5:-1].split(',')))[1:] # Height (cm) (converted to m in dataframe)\n",
    "                h = np.array(height) # Create array of height for conversion to depth \n",
    "                depth = (h-h[-1])*-1 # Depth sets surface as 0\n",
    "                density = list(map(float,data[index+2][5:-1].split(',')))[1:] # Density (kg/m^3)\n",
    "                temperature = list(map(float,data[index+3][5:-1].split(',')))[1:] # Temperature (dec C)\n",
    "                water = list(map(float,data[index+6][5:-1].split(',')))[1:] # Water content (%)\n",
    "                ice = list(map(float,data[index+14][5:-1].split(',')))[1:] # Ice content (%)\n",
    "                air = list(map(float,data[index+15][5:-1].split(',')))[1:] # Air content (%)\n",
    "                t = timestamp_sp\n",
    "                \n",
    "    dict_sp = {'ID':meta.VIR,'Timestamp':t,'Elevation':elevation_sp,'Latitude':latitude_sp,'Longitude':longitude_sp,'Height':np.array(height)/100,\n",
    "               'Depth':depth/100,'Density':np.array(density),'Temperature':np.array(temperature),'Ice':np.array(ice)/100,'Air':np.array(air)/100,'Water':np.array(water)/100}\n",
    "    \n",
    "    # Create SUMup dictionary for corresponding core\n",
    "    df_core = df[df.CoreID==meta.CoreID]\n",
    "    \n",
    "    coreid = df_core.CoreID.values[0]\n",
    "    citation_su = df_core.Citation.values[0]\n",
    "    timestamp_su = df_core.Timestamp.values[0]\n",
    "    latitude_su = df_core.Latitude.values[0]\n",
    "    longitude_su = df_core.Longitude.values[0]\n",
    "    elevation_su = df_core.Elevation.values[0]\n",
    "    \n",
    "    midpoint_su = np.array(df_core.Midpoint)\n",
    "    startdepth_su = np.array(df_core.StartDepth)\n",
    "    stopdepth_su = np.array(df_core.StopDepth)\n",
    "    thickness_su = np.array(df_core.Thickness)\n",
    "    density_su = np.array(df_core.Density)\n",
    "    \n",
    "    dict_su = {'CoreID':coreid,'Citation':citation_su,'Timestamp':timestamp_su,'Latitude':latitude_su,'Longitude':longitude_su,\n",
    "               'Elevation':elevation_su,'Midpoint':midpoint_su,'StartDepth':startdepth_su,'StopDepth':stopdepth_su,'Thickness':thickness_su,'Density':density_su}\n",
    "    \n",
    "    dictionaries = {'SNOWPACK':dict_sp,'SUMup':dict_su}\n",
    "    \n",
    "    dict_list.append(dictionaries)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_list, open(icesheet+'_data.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpack",
   "language": "python",
   "name": "snowpack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
