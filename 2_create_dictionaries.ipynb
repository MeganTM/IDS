{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionaries for SUMup and SNOWPACK\n",
    "\n",
    "#### Author: Megan Thompson-Munson\n",
    "#### Date created: 20 September 2021\n",
    "\n",
    "This script reads in SUMup observation data and SNOWPACK output, reformats the data, and creates dictionaries that are saved as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN\n",
    "\n",
    "# Select ice sheet\n",
    "icesheet = 'AIS'\n",
    "\n",
    "# Give path of SNOWPACK and CFM output data\n",
    "pathSP = '/projects/metm9666/snowpack/Scripts/Spinup/'\n",
    "pathCFM = '/scratch/summit/metm9666/CFMresults/'\n",
    "\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import pickle\n",
    "from scipy.interpolate import griddata\n",
    "from os.path import exists\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in SUMup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumup = pickle.load(open(icesheet+'_SUMup.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SU_ID</th>\n",
       "      <th>SU_Timestamp</th>\n",
       "      <th>SU_Latitude</th>\n",
       "      <th>SU_Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-12-28</td>\n",
       "      <td>-79.446800</td>\n",
       "      <td>-117.963501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-12-12</td>\n",
       "      <td>-79.347900</td>\n",
       "      <td>-116.290497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-12-24</td>\n",
       "      <td>-78.836899</td>\n",
       "      <td>-116.307098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2011-12-14</td>\n",
       "      <td>-78.727997</td>\n",
       "      <td>-114.732201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>-78.424301</td>\n",
       "      <td>-115.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1533.0</td>\n",
       "      <td>2009-02-04</td>\n",
       "      <td>-70.888000</td>\n",
       "      <td>133.285004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1534.0</td>\n",
       "      <td>2009-02-06</td>\n",
       "      <td>-69.827003</td>\n",
       "      <td>134.201996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>1535.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-68.014999</td>\n",
       "      <td>136.464005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>1536.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-67.415001</td>\n",
       "      <td>138.602005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>1537.0</td>\n",
       "      <td>2007-01-26</td>\n",
       "      <td>-71.408302</td>\n",
       "      <td>-9.916700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>894 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SU_ID SU_Timestamp  SU_Latitude  SU_Longitude\n",
       "0       0.0   2011-12-28   -79.446800   -117.963501\n",
       "1       1.0   2011-12-12   -79.347900   -116.290497\n",
       "2       2.0   2011-12-24   -78.836899   -116.307098\n",
       "3       3.0   2011-12-14   -78.727997   -114.732201\n",
       "4       4.0   2011-12-20   -78.424301   -115.292000\n",
       "..      ...          ...          ...           ...\n",
       "889  1533.0   2009-02-04   -70.888000    133.285004\n",
       "890  1534.0   2009-02-06   -69.827003    134.201996\n",
       "891  1535.0   2009-02-08   -68.014999    136.464005\n",
       "892  1536.0   2009-02-08   -67.415001    138.602005\n",
       "893  1537.0   2007-01-26   -71.408302     -9.916700\n",
       "\n",
       "[894 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SUMup metadata for finding closest MERRA-2 point\n",
    "suMeta = np.empty((len(sumup),4))\n",
    "\n",
    "# Loop through each core\n",
    "for i in range(len(sumup)):\n",
    "    \n",
    "    suData = sumup[i]\n",
    "\n",
    "    # Extract metadata \n",
    "    suMeta[i,0] = suData['CoreID']\n",
    "    suMeta[i,1] = suData['Timestamp']\n",
    "    suMeta[i,2] = suData['Latitude']\n",
    "    suMeta[i,3] = suData['Longitude']\n",
    "\n",
    "df_suMeta = pd.DataFrame(suMeta,columns=['SU_ID','SU_Timestamp','SU_Latitude','SU_Longitude'])\n",
    "df_suMeta['SU_Timestamp'] = pd.to_datetime(df_suMeta.SU_Timestamp)\n",
    "df_suMeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect SUMup and MERRA-2 metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SU_ID</th>\n",
       "      <th>SU_Timestamp</th>\n",
       "      <th>SU_Latitude</th>\n",
       "      <th>SU_Longitude</th>\n",
       "      <th>VIR</th>\n",
       "      <th>M2_Latitude</th>\n",
       "      <th>M2_Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-12-28</td>\n",
       "      <td>-79.446800</td>\n",
       "      <td>-117.963501</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-79.5</td>\n",
       "      <td>-118.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-12-12</td>\n",
       "      <td>-79.347900</td>\n",
       "      <td>-116.290497</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-79.5</td>\n",
       "      <td>-116.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-12-24</td>\n",
       "      <td>-78.836899</td>\n",
       "      <td>-116.307098</td>\n",
       "      <td>105.0</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>-116.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2011-12-14</td>\n",
       "      <td>-78.727997</td>\n",
       "      <td>-114.732201</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-78.5</td>\n",
       "      <td>-115.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>-78.424301</td>\n",
       "      <td>-115.292000</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-78.5</td>\n",
       "      <td>-115.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1533.0</td>\n",
       "      <td>2009-02-04</td>\n",
       "      <td>-70.888000</td>\n",
       "      <td>133.285004</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1534.0</td>\n",
       "      <td>2009-02-06</td>\n",
       "      <td>-69.827003</td>\n",
       "      <td>134.201996</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>1535.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-68.014999</td>\n",
       "      <td>136.464005</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>1536.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-67.415001</td>\n",
       "      <td>138.602005</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>1537.0</td>\n",
       "      <td>2007-01-26</td>\n",
       "      <td>-71.408302</td>\n",
       "      <td>-9.916700</td>\n",
       "      <td>250.0</td>\n",
       "      <td>-71.5</td>\n",
       "      <td>-10.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>894 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SU_ID SU_Timestamp  SU_Latitude  SU_Longitude    VIR  M2_Latitude  \\\n",
       "0       0.0   2011-12-28   -79.446800   -117.963501   93.0        -79.5   \n",
       "1       1.0   2011-12-12   -79.347900   -116.290497   94.0        -79.5   \n",
       "2       2.0   2011-12-24   -78.836899   -116.307098  105.0        -79.0   \n",
       "3       3.0   2011-12-14   -78.727997   -114.732201  120.0        -78.5   \n",
       "4       4.0   2011-12-20   -78.424301   -115.292000  120.0        -78.5   \n",
       "..      ...          ...          ...           ...    ...          ...   \n",
       "889  1533.0   2009-02-04   -70.888000    133.285004  141.0        -78.0   \n",
       "890  1534.0   2009-02-06   -69.827003    134.201996  141.0        -78.0   \n",
       "891  1535.0   2009-02-08   -68.014999    136.464005  141.0        -78.0   \n",
       "892  1536.0   2009-02-08   -67.415001    138.602005  141.0        -78.0   \n",
       "893  1537.0   2007-01-26   -71.408302     -9.916700  250.0        -71.5   \n",
       "\n",
       "     M2_Longitude  \n",
       "0        -118.125  \n",
       "1        -116.250  \n",
       "2        -116.250  \n",
       "3        -115.000  \n",
       "4        -115.000  \n",
       "..            ...  \n",
       "889       153.125  \n",
       "890       153.125  \n",
       "891       153.125  \n",
       "892       153.125  \n",
       "893       -10.000  \n",
       "\n",
       "[894 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in MERRA-2 location data from station lists\n",
    "if icesheet == 'AIS':\n",
    "    df_M2 = pd.read_table('AIS_station_list.lst',\n",
    "                          skiprows=1,delim_whitespace=True,usecols=[0,3,4],\n",
    "                          names=['Station','Latitude','Longitude'])\n",
    "if icesheet == 'GrIS':\n",
    "    df_M2 = pd.read_table('GrIS_station_list.lst',\n",
    "                          skiprows=1,delim_whitespace=True,usecols=[0,3,4],\n",
    "                          names=['Station','Latitude','Longitude'])\n",
    "\n",
    "# Extract VIRs (SNOWPACK IDs)\n",
    "VIRs = []\n",
    "for i in range(len(df_M2)):\n",
    "    VIR = df_M2.Station[i][-3:]\n",
    "    VIRs.append(VIR)\n",
    "df_M2['VIR'] = VIRs\n",
    "df_M2.drop(columns=['Station'])\n",
    "\n",
    "# Haversine formula for calculating distance between two points on Earth\n",
    "def haversine(lat1,lon1,lat2,lon2):\n",
    "    phi1 = np.deg2rad(lat1)\n",
    "    phi2 = np.deg2rad(lat2)\n",
    "    theta1 = np.deg2rad(lon1)\n",
    "    theta2 = np.deg2rad(lon2)\n",
    "    del_phi = phi2-phi1\n",
    "    del_theta = theta2-theta1\n",
    "    a = np.sin(del_phi/2)**2+(np.cos(phi1)*np.cos(phi2)*np.sin(del_theta/2)**2)\n",
    "    c = 2*np.arctan2(np.sqrt(a),np.sqrt(1-a))\n",
    "    d = (6371e3)*c # Earth's radius in meters\n",
    "    return d # Meters\n",
    "\n",
    "# Function for finding closest MERRA-2 location to given SUMup location\n",
    "def closest_location(sumuplat,sumuplon):\n",
    "    distance = []\n",
    "    for i in range(len(df_M2)):\n",
    "        lat1 = sumuplat\n",
    "        lon1 = sumuplon\n",
    "        lat2 = df_M2.Latitude[i]\n",
    "        lon2 = df_M2.Longitude[i]\n",
    "        d = haversine(lat1,lon1,lat2,lon2)\n",
    "        distance.append(d)\n",
    "    p = np.where(distance == min(distance))\n",
    "    return df_M2.loc[p]\n",
    "\n",
    "# Save metadata in 2-D array\n",
    "metadata = np.zeros((len(df_suMeta),7))\n",
    "for i in range(len(df_suMeta)):\n",
    "    metadata[i,0] = df_suMeta.SU_ID[i]\n",
    "    metadata[i,1] = np.array(df_suMeta.SU_Timestamp)[i]\n",
    "    metadata[i,2] = df_suMeta.SU_Latitude[i]\n",
    "    metadata[i,3] = df_suMeta.SU_Longitude[i]\n",
    "    merra2 = closest_location(df_suMeta.SU_Latitude[i],df_suMeta.SU_Longitude[i])\n",
    "    metadata[i,4] = merra2.VIR.values[0]\n",
    "    metadata[i,5] = merra2.Latitude.values[0]\n",
    "    metadata[i,6] = merra2.Longitude.values[0]\n",
    "\n",
    "# Create dataframe of SUMup and MERRA-2 metadata\n",
    "df_meta = pd.DataFrame(metadata,columns=['SU_ID','SU_Timestamp','SU_Latitude','SU_Longitude',\n",
    "                                         'VIR','M2_Latitude','M2_Longitude'])\n",
    "\n",
    "# Convert float back to timestamp and reset index\n",
    "df_meta['SU_Timestamp'] = pd.to_datetime(df_meta.SU_Timestamp)\n",
    "df_meta.reset_index(drop=True,inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SU_ID</th>\n",
       "      <th>SU_Timestamp</th>\n",
       "      <th>SU_Latitude</th>\n",
       "      <th>SU_Longitude</th>\n",
       "      <th>VIR</th>\n",
       "      <th>M2_Latitude</th>\n",
       "      <th>M2_Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-12-28</td>\n",
       "      <td>-79.446800</td>\n",
       "      <td>-117.963501</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-79.5</td>\n",
       "      <td>-118.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-12-12</td>\n",
       "      <td>-79.347900</td>\n",
       "      <td>-116.290497</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-79.5</td>\n",
       "      <td>-116.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-12-24</td>\n",
       "      <td>-78.836899</td>\n",
       "      <td>-116.307098</td>\n",
       "      <td>105.0</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>-116.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2011-12-14</td>\n",
       "      <td>-78.727997</td>\n",
       "      <td>-114.732201</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-78.5</td>\n",
       "      <td>-115.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>-78.424301</td>\n",
       "      <td>-115.292000</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-78.5</td>\n",
       "      <td>-115.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>1533.0</td>\n",
       "      <td>2009-02-04</td>\n",
       "      <td>-70.888000</td>\n",
       "      <td>133.285004</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>1534.0</td>\n",
       "      <td>2009-02-06</td>\n",
       "      <td>-69.827003</td>\n",
       "      <td>134.201996</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>1535.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-68.014999</td>\n",
       "      <td>136.464005</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>1536.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-67.415001</td>\n",
       "      <td>138.602005</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1537.0</td>\n",
       "      <td>2007-01-26</td>\n",
       "      <td>-71.408302</td>\n",
       "      <td>-9.916700</td>\n",
       "      <td>250.0</td>\n",
       "      <td>-71.5</td>\n",
       "      <td>-10.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SU_ID SU_Timestamp  SU_Latitude  SU_Longitude    VIR  M2_Latitude  \\\n",
       "0       0.0   2011-12-28   -79.446800   -117.963501   93.0        -79.5   \n",
       "1       1.0   2011-12-12   -79.347900   -116.290497   94.0        -79.5   \n",
       "2       2.0   2011-12-24   -78.836899   -116.307098  105.0        -79.0   \n",
       "3       3.0   2011-12-14   -78.727997   -114.732201  120.0        -78.5   \n",
       "4       4.0   2011-12-20   -78.424301   -115.292000  120.0        -78.5   \n",
       "..      ...          ...          ...           ...    ...          ...   \n",
       "875  1533.0   2009-02-04   -70.888000    133.285004  141.0        -78.0   \n",
       "876  1534.0   2009-02-06   -69.827003    134.201996  141.0        -78.0   \n",
       "877  1535.0   2009-02-08   -68.014999    136.464005  141.0        -78.0   \n",
       "878  1536.0   2009-02-08   -67.415001    138.602005  141.0        -78.0   \n",
       "879  1537.0   2007-01-26   -71.408302     -9.916700  250.0        -71.5   \n",
       "\n",
       "     M2_Longitude  \n",
       "0        -118.125  \n",
       "1        -116.250  \n",
       "2        -116.250  \n",
       "3        -115.000  \n",
       "4        -115.000  \n",
       "..            ...  \n",
       "875       153.125  \n",
       "876       153.125  \n",
       "877       153.125  \n",
       "878       153.125  \n",
       "879       -10.000  \n",
       "\n",
       "[880 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some SNOWPACK files don't exist (model errors), so check to see which ones don't and exclude them in the dataframe\n",
    "\n",
    "# Empty list for non-existent files\n",
    "nonfiles = []\n",
    "\n",
    "# Loop through all SNOWPACK output\n",
    "for i in range(len(df_M2)):\n",
    "    \n",
    "    # Extract the VIR\n",
    "    vir = df_M2.VIR.values[i]\n",
    "\n",
    "    # File paths\n",
    "    if icesheet == 'AIS':\n",
    "        file = pathSP+'AIS_SUMup_output/VIR'+vir+'_AIS_SUMup.pro'    \n",
    "    if icesheet == 'GrIS':\n",
    "        file = pathSP+'GrIS_SUMup_output/VIR'+vir+'_GrIS_SUMup.pro'    \n",
    "        \n",
    "    # If the file does not exist, add to lsit\n",
    "    if exists(file) == False:\n",
    "        nonfiles.append(int(vir))\n",
    "\n",
    "# Ignore any missing files\n",
    "df_meta = df_meta[~df_meta['VIR'].isin(nonfiles)]\n",
    "df_meta.reset_index(drop=True,inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create dictionaries with SNOWPACK, CFM, and SUMup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert CFM dates to timestamps (e.g., 1980.0027 --> 1980-1-1)\n",
    "def decimal2datetime(decimalYear):\n",
    "    ts = []\n",
    "    for i in range(len(decimalYear)):\n",
    "        date = decimalYear[i]\n",
    "        year = int(date)\n",
    "        decimal = date - year\n",
    "        days = decimal*365.25\n",
    "        timestamp = pd.to_datetime(datetime.datetime(year,1,1)+datetime.timedelta(days))\n",
    "        ts.append(timestamp)\n",
    "    return ts\n",
    "\n",
    "# List for storing dictionaries\n",
    "# dict_list = []\n",
    "\n",
    "# Loop through each SUMup core\n",
    "# for i in range(len(df_meta)):\n",
    "# for i in range(0,440):\n",
    "# for i in range(440,880):\n",
    "    \n",
    "    # Meta information about SUMup and MERRA-2\n",
    "    meta = df_meta.loc[i]\n",
    "\n",
    "    # Get VIR and convert to appropriate string for file path\n",
    "    vir = str(int(meta.VIR))\n",
    "    if len(vir) == 1:\n",
    "        strvir = '00'+vir\n",
    "    if len(vir) == 2:\n",
    "        strvir = '0'+vir\n",
    "    if len(vir) == 3:\n",
    "        strvir = vir\n",
    "    \n",
    "    # SNOWPACK .pro and .smet files\n",
    "    if icesheet == 'AIS':\n",
    "        proFile = pathSP+'AIS_SUMup_output/VIR'+strvir+'_AIS_SUMup.pro'\n",
    "        smetFile = pathSP+'AIS_SUMup_output/VIR'+strvir+'_AIS_SUMup.smet'\n",
    "    if icesheet == 'GrIS':\n",
    "        proFile = pathSP+'GrIS_SUMup_output/VIR'+strvir+'_GrIS_SUMup.pro'\n",
    "        smetFile = pathSP+'GrIS_SUMup_output/VIR'+strvir+'_GrIS_SUMup.smet'\n",
    "        \n",
    "    \n",
    "### SNOWPACK pro file ###\n",
    "    \n",
    "    # Open *.pro file and read in header (44 lines in length)\n",
    "    proF = open(proFile,'r')\n",
    "    for j in range(44):\n",
    "        header = proF.readline()\n",
    "        if j == 1:\n",
    "            VIR = int(header[29:-1]) # Should be same as meta data\n",
    "        if j == 2:\n",
    "            SP_Latitude = float(header[10:-1]) # Should be same as meta data\n",
    "        if j == 3:\n",
    "            SP_Longitude = float(header[11:-1]) # Should be same as meta data\n",
    "        if j == 4:\n",
    "            SP_Elevation = float(header[9:-1])\n",
    "    \n",
    "    SP_Timestamps = [] # Empty list for storing SNOWPACK timestamps\n",
    "\n",
    "    # Read data line by line\n",
    "    SP_data = proF.readlines()\n",
    "    for line in SP_data:\n",
    "        linecode = line[0:4] # SNOWPACK gives each data type a 4-digit code\n",
    "\n",
    "        # Extract timestamps and save in a list\n",
    "        if linecode == '0500':\n",
    "            SP_RawDate = line[5:24]\n",
    "            SP_Date = datetime.datetime.strptime(SP_RawDate,'%d.%m.%Y %H:%M:%S')\n",
    "            SP_Timestamp = pd.to_datetime(SP_Date)\n",
    "            SP_Timestamps.append(SP_Timestamp)\n",
    "\n",
    "    # Find SNOWPACK timestamp that's closest to the desired SUMup one\n",
    "    SP_closest = min(SP_Timestamps, key=lambda sub: abs(sub - meta.SU_Timestamp))\n",
    "    k = np.where(np.array(SP_Timestamps)==SP_closest)[0][0]\n",
    "\n",
    "    # Read data and extract lines corresponding to closest timestamp\n",
    "    for line in SP_data:\n",
    "        linecode = line[0:4] # SNOWPACK gives each data type a 4-digit code\n",
    "\n",
    "        if linecode == '0500':\n",
    "            SP_RawDate = line[5:24]\n",
    "            SP_Date = datetime.datetime.strptime(SP_RawDate,'%d.%m.%Y %H:%M:%S')\n",
    "            SP_Timestamp = pd.to_datetime(SP_Date)\n",
    "\n",
    "            if SP_Timestamp == SP_closest:\n",
    "\n",
    "                index = k*27 # Each timestamp has 27 elements, so this allows us to get to the start of each new timestamp\n",
    "\n",
    "                # Extract variables of interest by spliting the lines and creating lists of the data\n",
    "                SP_height = list(map(float,SP_data[index+1][5:-1].split(',')))[1:] # Height (cm) (converted to m in dataframe)\n",
    "                SP_h = np.array(SP_height) # Create array of height for conversion to depth \n",
    "                SP_depth = (SP_h-SP_h[-1])*-1 # Depth sets surface as 0\n",
    "                SP_density = list(map(float,SP_data[index+2][5:-1].split(',')))[1:] # Density (kg/m^3)\n",
    "                SP_temperature = list(map(float,SP_data[index+3][5:-1].split(',')))[1:] # Temperature (dec C)\n",
    "                SP_water = list(map(float,SP_data[index+6][5:-1].split(',')))[1:] # Water content (%)\n",
    "                SP_ice = list(map(float,SP_data[index+14][5:-1].split(',')))[1:] # Ice content (%)\n",
    "                SP_air = list(map(float,SP_data[index+15][5:-1].split(',')))[1:] # Air content (%)\n",
    "                t = SP_Timestamp # Grab the correct timestamp\n",
    "                \n",
    "    SP_dict = {'VIR':meta.VIR,'Timestamp':t,'Elevation':SP_Elevation,'Latitude':SP_Latitude,'Longitude':SP_Longitude,'Height':np.array(SP_height)/100,\n",
    "              'Depth':SP_depth/100,'Density':np.array(SP_density),'Temperature':np.array(SP_temperature),\n",
    "              'Ice':np.array(SP_ice)/100,'Air':np.array(SP_air)/100,'Water':np.array(SP_water)/100}\n",
    "    \n",
    "    proF.close()\n",
    "\n",
    "### SNOWPACK smet file ###\n",
    "    \n",
    "    # Get smet info from header\n",
    "    df_smetinfo = pd.read_table(smetFile,skiprows=9,nrows=8,delim_whitespace=True,header=None)\n",
    "    df_smetinfo.reset_index(drop=True,inplace=True)\n",
    "    df_smetinfo.columns = np.array(df_smetinfo.iloc[5])\n",
    "    df_smetinfo = df_smetinfo.drop([2,3,5,6])\n",
    "    df_smetinfo = df_smetinfo.drop(columns=['='])\n",
    "    df_smetinfo.set_index('plot_description',inplace=True)\n",
    "    \n",
    "    # Read in smet file and create arrays of relevant data\n",
    "    df_smet = pd.read_table(smetFile,skiprows=18,delim_whitespace=True,names=np.array(df_smetinfo.iloc[3]))\n",
    "    smetTimestamp = np.array(pd.to_datetime(df_smet.timestamp))\n",
    "    smetSnow = np.array(df_smet.MS_Snow)\n",
    "    smetWind = np.array(df_smet.MS_Wind)\n",
    "    smetRain = np.array(df_smet.MS_Rain)\n",
    "    smetTemp = np.array(df_smet.TA)\n",
    "    smetMelt = np.array(df_smet.MS_melt)\n",
    "    smetEvap = np.array(df_smet.MS_Evap)\n",
    "    smetSubl = np.array(df_smet.MS_Sublimation)\n",
    "    smetRunoff = np.array(df_smet.MS_SN_Runoff)\n",
    "    smetSWE = np.array(df_smet.SWE)\n",
    "    \n",
    "    # Create smet dictionary\n",
    "    smet_dict = {'VIR':meta.VIR,'Latitude':meta.M2_Latitude,'Longitude':meta.M2_Longitude,'Timestamp':smetTimestamp,'Temperature':smetTemp,'Snow':smetSnow,\n",
    "                'Rain':smetRain,'Melt':smetMelt,'Wind':smetWind,'Evaporation':smetEvap,'Sublimation':smetSubl,'Runoff':smetRunoff,'SWE':smetSWE}\n",
    "\n",
    "### CFM ###\n",
    "    \n",
    "    # Get MERRA-2 coordinates from metadata for CFM file name\n",
    "    M2_lat = meta.M2_Latitude\n",
    "    M2_lon = meta.M2_Longitude\n",
    "\n",
    "    # This longitude was manually changed to 0 for SNOWPACK runs, but need to be reverted back to e-13 for CFM\n",
    "    if (M2_lat==-75.0) & (M2_lon==0.0):\n",
    "        M2_lon = -5.920304394294029e-13\n",
    "\n",
    "    # Get CFM file for corresponding coordinate and read it in\n",
    "    CFM_file = pathCFM + 'IDS_baseline_{}_{}_1D_mean'.format(M2_lat,M2_lon) + '/CFMresults.hdf5'\n",
    "    CFM_f = xr.open_dataset(CFM_file)\n",
    "\n",
    "    # Get times and convert decimal years to timestamp\n",
    "    CFM_timesteps = CFM_f['density'][1:,0]\n",
    "    CFM_timestamps = decimal2datetime(np.array(CFM_timesteps))\n",
    "\n",
    "    # Find closes CFM timestamp to SUMup timestamp\n",
    "    CFM_closest = min(CFM_timestamps, key=lambda sub: abs(sub - meta.SU_Timestamp))\n",
    "    l = np.where(np.array(CFM_timestamps)==CFM_closest)[0][0]\n",
    "    \n",
    "    # Get variables of interest\n",
    "    CFM_depth = CFM_f['depth'][1:,1:][l].values\n",
    "    CFM_density = CFM_f['density'][1:,1:][l].values\n",
    "    CFM_dip = CFM_f['DIP'][1:,1:][l].values\n",
    "    CFM_f.close()\n",
    "\n",
    "    CFM_dict = {'Latitude':M2_lat,'Longitude':M2_lon,'Timestamp':CFM_timestamps[l],\n",
    "                'Depth':CFM_depth,'Density':CFM_density,'DIP':CFM_dip}\n",
    "\n",
    "### SUMup ###\n",
    "    \n",
    "    # Extract SUMup dictionary for corresponding SNOWPACK data\n",
    "    SU_DictIndex = df_suMeta[df_suMeta.SU_ID==meta.SU_ID].index[0]\n",
    "    SU_dict = sumup[SU_DictIndex]\n",
    "    \n",
    "    # Create dictionary and append list\n",
    "    dictionaries = {'MERRA-2':smet_dict, 'SNOWPACK':SP_dict,\n",
    "                    'CFM':CFM_dict, 'SUMup':SU_dict}\n",
    "\n",
    "    dict_list.append(dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_list, open(icesheet+'_data.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create interpolated dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(icesheet+'_data.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dictionaries\n",
    "dict_list = []\n",
    "\n",
    "# Regrid models on to observations' vertical scales\n",
    "for i in range(len(data)):\n",
    "\n",
    "    # Select SUMup, CFM, and SNOWPACK keys\n",
    "    SU = data[i]['SUMup']\n",
    "    SP = data[i]['SNOWPACK']\n",
    "    CFM = data[i]['CFM']\n",
    "    \n",
    "    # Get relevant variables from dictionaries\n",
    "    SU_density = SU['Density']\n",
    "    SU_depth = SU['Midpoint']\n",
    "    SP_depth = SP['Depth']\n",
    "    SP_density = SP['Density']\n",
    "    SP_air = SP['Air']\n",
    "    SP_water = SP['Water']\n",
    "    SP_ice = SP['Ice']\n",
    "    CFM_depth = CFM['Depth']\n",
    "    CFM_density = CFM['Density']\n",
    "    CFM_dip = CFM['DIP']\n",
    "\n",
    "    \n",
    "    # Many SUMup measurements are just single point, so just regrid datasets with n > 1\n",
    "    if len(SU_density) > 1:\n",
    "        \n",
    "        # Interpolate modeled density onto observations\n",
    "        SU_densityInt = SU_density\n",
    "        SP_densityInt = griddata(SP_depth,SP_density,SU_depth)\n",
    "        CFM_densityInt = griddata(CFM_depth,CFM_density,SU_depth)\n",
    "        \n",
    "        # Filter out nans\n",
    "        nanfilter = (~np.isnan(SU_densityInt)) & (~np.isnan(SP_densityInt)) & (~np.isnan(CFM_densityInt))\n",
    "        SU_densityFilt = SU_densityInt[nanfilter]\n",
    "        SP_densityFilt = SP_densityInt[nanfilter]\n",
    "        CFM_densityFilt = CFM_densityInt[nanfilter]\n",
    "        \n",
    "        dictionaries = {'SNOWPACK':{'VIR':SP['VIR'],'Timestamp':SP['Timestamp'],\n",
    "                                    'Latitude':SP['Latitude'],'Longitude':SP['Longitude'],\n",
    "                                    'Elevation':SP['Elevation'],'Depth':SU_depth,'Density':SP_densityFilt},\n",
    "                        'CFM':{'Timestamp':CFM['Timestamp'],'Latitude':CFM['Latitude'],'Longitude':CFM['Longitude'],\n",
    "                                    'Elevation':SP['Elevation'],'Depth':SU_depth,'Density':CFM_densityFilt},\n",
    "                        'SUMup':{'CoreID':SU['CoreID'],'Citation':SU['Citation'],'Timestamp':SU['Timestamp'],\n",
    "                                 'Latitude':SU['Latitude'],'Longitude':SU['Longitude'],\n",
    "                                 'Elevation':SU['Elevation'],'Depth':SU_depth,'Density':SU_densityFilt}}\n",
    "    \n",
    "    else:\n",
    "        dictionaries = {'SNOWPACK':{'VIR':SP['VIR'],'Timestamp':SP['Timestamp'],\n",
    "                            'Latitude':SP['Latitude'],'Longitude':SP['Longitude'],\n",
    "                            'Elevation':SP['Elevation'],'Depth':SP_depth,'Density':SP_density},\n",
    "                        'CFM':{'Timestamp':CFM['Timestamp'],'Latitude':CFM['Latitude'],'Longitude':CFM['Longitude'],\n",
    "                                    'Elevation':SP['Elevation'],'Depth':CFM_depth,'Density':CFM_density},\n",
    "                        'SUMup':{'CoreID':SU['CoreID'],'Citation':SU['Citation'],'Timestamp':SU['Timestamp'],\n",
    "                                 'Latitude':SU['Latitude'],'Longitude':SU['Longitude'],\n",
    "                                 'Elevation':SU['Elevation'],'Depth':SU_depth,'Density':SU_density}}\n",
    "\n",
    "    dict_list.append(dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_list, open(icesheet+'_data_interpolated.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpack",
   "language": "python",
   "name": "snowpack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
