{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionaries for SUMup and SNOWPACK\n",
    "\n",
    "#### Author: Megan Thompson-Munson\n",
    "#### Date created: 20 September 2021\n",
    "\n",
    "This script reads in SUMup observation data and SNOWPACK output, reformats the data, and creates dictionaries that are saved as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN\n",
    "\n",
    "# Select ice sheet\n",
    "icesheet = 'GrIS'\n",
    "\n",
    "# Give path of SNOWPACK and CFM output data\n",
    "pathSP = '/projects/metm9666/snowpack/Scripts/Spinup/'\n",
    "pathCFM = '/scratch/summit/metm9666/CFMresults/'\n",
    "\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import pickle\n",
    "from scipy.interpolate import griddata\n",
    "from os.path import exists\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in SUMup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumup = pickle.load(open(icesheet+'_SUMup.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SU_ID</th>\n",
       "      <th>SU_Timestamp</th>\n",
       "      <th>SU_Latitude</th>\n",
       "      <th>SU_Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>73.647102</td>\n",
       "      <td>-38.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2011-05-24</td>\n",
       "      <td>74.507965</td>\n",
       "      <td>-41.339031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>75.129303</td>\n",
       "      <td>-40.541832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2010-05-20</td>\n",
       "      <td>75.408035</td>\n",
       "      <td>-41.068619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2010-05-20</td>\n",
       "      <td>75.949409</td>\n",
       "      <td>-42.160610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>1571.0</td>\n",
       "      <td>2008-06-08</td>\n",
       "      <td>72.549934</td>\n",
       "      <td>-38.309067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>1572.0</td>\n",
       "      <td>2008-06-16</td>\n",
       "      <td>72.579552</td>\n",
       "      <td>-38.505466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>1573.0</td>\n",
       "      <td>2008-06-20</td>\n",
       "      <td>72.549934</td>\n",
       "      <td>-38.309067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>1574.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.635132</td>\n",
       "      <td>-38.514919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>1575.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.579781</td>\n",
       "      <td>-38.458630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>682 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SU_ID SU_Timestamp  SU_Latitude  SU_Longitude\n",
       "0       6.0   2011-05-12    73.647102    -38.677200\n",
       "1       7.0   2011-05-24    74.507965    -41.339031\n",
       "2       8.0   2011-05-12    75.129303    -40.541832\n",
       "3       9.0   2010-05-20    75.408035    -41.068619\n",
       "4      10.0   2010-05-20    75.949409    -42.160610\n",
       "..      ...          ...          ...           ...\n",
       "677  1571.0   2008-06-08    72.549934    -38.309067\n",
       "678  1572.0   2008-06-16    72.579552    -38.505466\n",
       "679  1573.0   2008-06-20    72.549934    -38.309067\n",
       "680  1574.0   2013-06-04    72.635132    -38.514919\n",
       "681  1575.0   2013-06-04    72.579781    -38.458630\n",
       "\n",
       "[682 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SUMup metadata for finding closest MERRA-2 point\n",
    "suMeta = np.empty((len(sumup),4))\n",
    "\n",
    "# Loop through each core\n",
    "for i in range(len(sumup)):\n",
    "    \n",
    "    suData = sumup[i]\n",
    "\n",
    "    # Extract metadata \n",
    "    suMeta[i,0] = suData['CoreID']\n",
    "    suMeta[i,1] = suData['Timestamp']\n",
    "    suMeta[i,2] = suData['Latitude']\n",
    "    suMeta[i,3] = suData['Longitude']\n",
    "\n",
    "df_suMeta = pd.DataFrame(suMeta,columns=['SU_ID','SU_Timestamp','SU_Latitude','SU_Longitude'])\n",
    "df_suMeta['SU_Timestamp'] = pd.to_datetime(df_suMeta.SU_Timestamp)\n",
    "df_suMeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect SUMup and MERRA-2 metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SU_ID</th>\n",
       "      <th>SU_Timestamp</th>\n",
       "      <th>SU_Latitude</th>\n",
       "      <th>SU_Longitude</th>\n",
       "      <th>VIR</th>\n",
       "      <th>M2_Latitude</th>\n",
       "      <th>M2_Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>73.647102</td>\n",
       "      <td>-38.677200</td>\n",
       "      <td>347.0</td>\n",
       "      <td>73.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2011-05-24</td>\n",
       "      <td>74.507965</td>\n",
       "      <td>-41.339031</td>\n",
       "      <td>351.0</td>\n",
       "      <td>74.5</td>\n",
       "      <td>-41.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>75.129303</td>\n",
       "      <td>-40.541832</td>\n",
       "      <td>355.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-40.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2010-05-20</td>\n",
       "      <td>75.408035</td>\n",
       "      <td>-41.068619</td>\n",
       "      <td>359.0</td>\n",
       "      <td>75.5</td>\n",
       "      <td>-41.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2010-05-20</td>\n",
       "      <td>75.949409</td>\n",
       "      <td>-42.160610</td>\n",
       "      <td>365.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-41.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>1571.0</td>\n",
       "      <td>2008-06-08</td>\n",
       "      <td>72.549934</td>\n",
       "      <td>-38.309067</td>\n",
       "      <td>339.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>1572.0</td>\n",
       "      <td>2008-06-16</td>\n",
       "      <td>72.579552</td>\n",
       "      <td>-38.505466</td>\n",
       "      <td>338.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>1573.0</td>\n",
       "      <td>2008-06-20</td>\n",
       "      <td>72.549934</td>\n",
       "      <td>-38.309067</td>\n",
       "      <td>339.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>1574.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.635132</td>\n",
       "      <td>-38.514919</td>\n",
       "      <td>338.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>1575.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.579781</td>\n",
       "      <td>-38.458630</td>\n",
       "      <td>338.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>682 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SU_ID SU_Timestamp  SU_Latitude  SU_Longitude    VIR  M2_Latitude  \\\n",
       "0       6.0   2011-05-12    73.647102    -38.677200  347.0         73.5   \n",
       "1       7.0   2011-05-24    74.507965    -41.339031  351.0         74.5   \n",
       "2       8.0   2011-05-12    75.129303    -40.541832  355.0         75.0   \n",
       "3       9.0   2010-05-20    75.408035    -41.068619  359.0         75.5   \n",
       "4      10.0   2010-05-20    75.949409    -42.160610  365.0         76.0   \n",
       "..      ...          ...          ...           ...    ...          ...   \n",
       "677  1571.0   2008-06-08    72.549934    -38.309067  339.0         72.5   \n",
       "678  1572.0   2008-06-16    72.579552    -38.505466  338.0         72.5   \n",
       "679  1573.0   2008-06-20    72.549934    -38.309067  339.0         72.5   \n",
       "680  1574.0   2013-06-04    72.635132    -38.514919  338.0         72.5   \n",
       "681  1575.0   2013-06-04    72.579781    -38.458630  338.0         72.5   \n",
       "\n",
       "     M2_Longitude  \n",
       "0         -38.750  \n",
       "1         -41.250  \n",
       "2         -40.625  \n",
       "3         -41.250  \n",
       "4         -41.875  \n",
       "..            ...  \n",
       "677       -38.125  \n",
       "678       -38.750  \n",
       "679       -38.125  \n",
       "680       -38.750  \n",
       "681       -38.750  \n",
       "\n",
       "[682 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in MERRA-2 location data from station lists\n",
    "if icesheet == 'AIS':\n",
    "    df_M2 = pd.read_table('AIS_station_list.lst',\n",
    "                          skiprows=1,delim_whitespace=True,usecols=[0,3,4],\n",
    "                          names=['Station','Latitude','Longitude'])\n",
    "if icesheet == 'GrIS':\n",
    "    df_M2 = pd.read_table('GrIS_station_list.lst',\n",
    "                          skiprows=1,delim_whitespace=True,usecols=[0,3,4],\n",
    "                          names=['Station','Latitude','Longitude'])\n",
    "\n",
    "# Extract VIRs (SNOWPACK IDs)\n",
    "VIRs = []\n",
    "for i in range(len(df_M2)):\n",
    "    VIR = df_M2.Station[i][-3:]\n",
    "    VIRs.append(VIR)\n",
    "df_M2['VIR'] = VIRs\n",
    "df_M2.drop(columns=['Station'])\n",
    "\n",
    "# Haversine formula for calculating distance between two points on Earth\n",
    "def haversine(lat1,lon1,lat2,lon2):\n",
    "    phi1 = np.deg2rad(lat1)\n",
    "    phi2 = np.deg2rad(lat2)\n",
    "    theta1 = np.deg2rad(lon1)\n",
    "    theta2 = np.deg2rad(lon2)\n",
    "    del_phi = phi2-phi1\n",
    "    del_theta = theta2-theta1\n",
    "    a = np.sin(del_phi/2)**2+(np.cos(phi1)*np.cos(phi2)*np.sin(del_theta/2)**2)\n",
    "    c = 2*np.arctan2(np.sqrt(a),np.sqrt(1-a))\n",
    "    d = (6371e3)*c # Earth's radius in meters\n",
    "    return d # Meters\n",
    "\n",
    "# Function for finding closest MERRA-2 location to given SUMup location\n",
    "def closest_location(sumuplat,sumuplon):\n",
    "    distance = []\n",
    "    for i in range(len(df_M2)):\n",
    "        lat1 = sumuplat\n",
    "        lon1 = sumuplon\n",
    "        lat2 = df_M2.Latitude[i]\n",
    "        lon2 = df_M2.Longitude[i]\n",
    "        d = haversine(lat1,lon1,lat2,lon2)\n",
    "        distance.append(d)\n",
    "    p = np.where(distance == min(distance))\n",
    "    return df_M2.loc[p]\n",
    "\n",
    "# Save metadata in 2-D array\n",
    "metadata = np.zeros((len(df_suMeta),7))\n",
    "for i in range(len(df_suMeta)):\n",
    "    metadata[i,0] = df_suMeta.SU_ID[i]\n",
    "    metadata[i,1] = np.array(df_suMeta.SU_Timestamp)[i]\n",
    "    metadata[i,2] = df_suMeta.SU_Latitude[i]\n",
    "    metadata[i,3] = df_suMeta.SU_Longitude[i]\n",
    "    merra2 = closest_location(df_suMeta.SU_Latitude[i],df_suMeta.SU_Longitude[i])\n",
    "    metadata[i,4] = merra2.VIR.values[0]\n",
    "    metadata[i,5] = merra2.Latitude.values[0]\n",
    "    metadata[i,6] = merra2.Longitude.values[0]\n",
    "\n",
    "# Create dataframe of SUMup and MERRA-2 metadata\n",
    "df_meta = pd.DataFrame(metadata,columns=['SU_ID','SU_Timestamp','SU_Latitude','SU_Longitude',\n",
    "                                         'VIR','M2_Latitude','M2_Longitude'])\n",
    "\n",
    "# Convert float back to timestamp and reset index\n",
    "df_meta['SU_Timestamp'] = pd.to_datetime(df_meta.SU_Timestamp)\n",
    "df_meta.reset_index(drop=True,inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SU_ID</th>\n",
       "      <th>SU_Timestamp</th>\n",
       "      <th>SU_Latitude</th>\n",
       "      <th>SU_Longitude</th>\n",
       "      <th>VIR</th>\n",
       "      <th>M2_Latitude</th>\n",
       "      <th>M2_Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>73.647102</td>\n",
       "      <td>-38.677200</td>\n",
       "      <td>347.0</td>\n",
       "      <td>73.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2011-05-24</td>\n",
       "      <td>74.507965</td>\n",
       "      <td>-41.339031</td>\n",
       "      <td>351.0</td>\n",
       "      <td>74.5</td>\n",
       "      <td>-41.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>75.129303</td>\n",
       "      <td>-40.541832</td>\n",
       "      <td>355.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-40.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2010-05-20</td>\n",
       "      <td>75.408035</td>\n",
       "      <td>-41.068619</td>\n",
       "      <td>359.0</td>\n",
       "      <td>75.5</td>\n",
       "      <td>-41.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2010-05-20</td>\n",
       "      <td>75.949409</td>\n",
       "      <td>-42.160610</td>\n",
       "      <td>365.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-41.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>1571.0</td>\n",
       "      <td>2008-06-08</td>\n",
       "      <td>72.549934</td>\n",
       "      <td>-38.309067</td>\n",
       "      <td>339.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1572.0</td>\n",
       "      <td>2008-06-16</td>\n",
       "      <td>72.579552</td>\n",
       "      <td>-38.505466</td>\n",
       "      <td>338.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>1573.0</td>\n",
       "      <td>2008-06-20</td>\n",
       "      <td>72.549934</td>\n",
       "      <td>-38.309067</td>\n",
       "      <td>339.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>1574.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.635132</td>\n",
       "      <td>-38.514919</td>\n",
       "      <td>338.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>1575.0</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>72.579781</td>\n",
       "      <td>-38.458630</td>\n",
       "      <td>338.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>-38.750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SU_ID SU_Timestamp  SU_Latitude  SU_Longitude    VIR  M2_Latitude  \\\n",
       "0       6.0   2011-05-12    73.647102    -38.677200  347.0         73.5   \n",
       "1       7.0   2011-05-24    74.507965    -41.339031  351.0         74.5   \n",
       "2       8.0   2011-05-12    75.129303    -40.541832  355.0         75.0   \n",
       "3       9.0   2010-05-20    75.408035    -41.068619  359.0         75.5   \n",
       "4      10.0   2010-05-20    75.949409    -42.160610  365.0         76.0   \n",
       "..      ...          ...          ...           ...    ...          ...   \n",
       "602  1571.0   2008-06-08    72.549934    -38.309067  339.0         72.5   \n",
       "603  1572.0   2008-06-16    72.579552    -38.505466  338.0         72.5   \n",
       "604  1573.0   2008-06-20    72.549934    -38.309067  339.0         72.5   \n",
       "605  1574.0   2013-06-04    72.635132    -38.514919  338.0         72.5   \n",
       "606  1575.0   2013-06-04    72.579781    -38.458630  338.0         72.5   \n",
       "\n",
       "     M2_Longitude  \n",
       "0         -38.750  \n",
       "1         -41.250  \n",
       "2         -40.625  \n",
       "3         -41.250  \n",
       "4         -41.875  \n",
       "..            ...  \n",
       "602       -38.125  \n",
       "603       -38.750  \n",
       "604       -38.125  \n",
       "605       -38.750  \n",
       "606       -38.750  \n",
       "\n",
       "[607 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some SNOWPACK files don't exist (model errors), so check to see which ones don't and exclude them in the dataframe\n",
    "\n",
    "# Empty list for non-existent files\n",
    "nonfiles = []\n",
    "\n",
    "# Loop through all SNOWPACK output\n",
    "for i in range(len(df_M2)):\n",
    "    \n",
    "    # Extract the VIR\n",
    "    vir = df_M2.VIR.values[i]\n",
    "\n",
    "    # File paths\n",
    "    if icesheet == 'AIS':\n",
    "        file = pathSP+'AIS_SUMup_output/VIR'+vir+'_AIS_SUMup.pro'    \n",
    "    if icesheet == 'GrIS':\n",
    "        file = pathSP+'GrIS_SUMup_output/VIR'+vir+'_GrIS_SUMup.pro'    \n",
    "        \n",
    "    # If the file does not exist, add to lsit\n",
    "    if exists(file) == False:\n",
    "        nonfiles.append(int(vir))\n",
    "\n",
    "# Ignore any missing files\n",
    "df_meta = df_meta[~df_meta['VIR'].isin(nonfiles)]\n",
    "df_meta.reset_index(drop=True,inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create dictionaries with SNOWPACK, CFM, and SUMup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert CFM dates to timestamps (e.g., 1980.0027 --> 1980-1-1)\n",
    "def decimal2datetime(decimalYear):\n",
    "    ts = []\n",
    "    for i in range(len(decimalYear)):\n",
    "        date = decimalYear[i]\n",
    "        year = int(date)\n",
    "        decimal = date - year\n",
    "        days = decimal*365.25\n",
    "        timestamp = pd.to_datetime(datetime.datetime(year,1,1)+datetime.timedelta(days))\n",
    "        ts.append(timestamp)\n",
    "    return ts\n",
    "\n",
    "# List for storing dictionaries\n",
    "dict_list = []\n",
    "\n",
    "# Loop through each SUMup core\n",
    "for i in range(0,200):\n",
    "    \n",
    "    # Meta information about SUMup and MERRA-2\n",
    "    meta = df_meta.loc[i]\n",
    "\n",
    "    # Get VIR and convert to appropriate string for file path\n",
    "    vir = str(int(meta.VIR))\n",
    "    if len(vir) == 1:\n",
    "        strvir = '00'+vir\n",
    "    if len(vir) == 2:\n",
    "        strvir = '0'+vir\n",
    "    if len(vir) == 3:\n",
    "        strvir = vir\n",
    "    \n",
    "    # SNOWPACK .pro and .smet files\n",
    "    if icesheet == 'AIS':\n",
    "        proFile = pathSP+'AIS_SUMup_output/VIR'+strvir+'_AIS_SUMup.pro'\n",
    "        smetFile = pathSP+'AIS_SUMup_output/VIR'+strvir+'_AIS_SUMup.smet'\n",
    "    if icesheet == 'GrIS':\n",
    "        proFile = pathSP+'GrIS_SUMup_output/VIR'+strvir+'_GrIS_SUMup.pro'\n",
    "        smetFile = pathSP+'GrIS_SUMup_output/VIR'+strvir+'_GrIS_SUMup.smet'\n",
    "        \n",
    "    \n",
    "### SNOWPACK pro file ###\n",
    "    \n",
    "    # Open *.pro file and read in header (44 lines in length)\n",
    "    proF = open(proFile,'r')\n",
    "    for j in range(44):\n",
    "        header = proF.readline()\n",
    "        if j == 1:\n",
    "            VIR = int(header[29:-1]) # Should be same as meta data\n",
    "        if j == 2:\n",
    "            SP_Latitude = float(header[10:-1]) # Should be same as meta data\n",
    "        if j == 3:\n",
    "            SP_Longitude = float(header[11:-1]) # Should be same as meta data\n",
    "        if j == 4:\n",
    "            SP_Elevation = float(header[9:-1])\n",
    "    \n",
    "    SP_Timestamps = [] # Empty list for storing SNOWPACK timestamps\n",
    "\n",
    "    # Read data line by line\n",
    "    SP_data = proF.readlines()\n",
    "    for line in SP_data:\n",
    "        linecode = line[0:4] # SNOWPACK gives each data type a 4-digit code\n",
    "\n",
    "        # Extract timestamps and save in a list\n",
    "        if linecode == '0500':\n",
    "            SP_RawDate = line[5:24]\n",
    "            SP_Date = datetime.datetime.strptime(SP_RawDate,'%d.%m.%Y %H:%M:%S')\n",
    "            SP_Timestamp = pd.to_datetime(SP_Date)\n",
    "            SP_Timestamps.append(SP_Timestamp)\n",
    "\n",
    "    # Find SNOWPACK timestamp that's closest to the desired SUMup one\n",
    "    SP_closest = min(SP_Timestamps, key=lambda sub: abs(sub - meta.SU_Timestamp))\n",
    "    k = np.where(np.array(SP_Timestamps)==SP_closest)[0][0]\n",
    "\n",
    "    # Read data and extract lines corresponding to closest timestamp\n",
    "    for line in SP_data:\n",
    "        linecode = line[0:4] # SNOWPACK gives each data type a 4-digit code\n",
    "\n",
    "        if linecode == '0500':\n",
    "            SP_RawDate = line[5:24]\n",
    "            SP_Date = datetime.datetime.strptime(SP_RawDate,'%d.%m.%Y %H:%M:%S')\n",
    "            SP_Timestamp = pd.to_datetime(SP_Date)\n",
    "\n",
    "            if SP_Timestamp == SP_closest:\n",
    "\n",
    "                index = k*27 # Each timestamp has 27 elements, so this allows us to get to the start of each new timestamp\n",
    "\n",
    "                # Extract variables of interest by spliting the lines and creating lists of the data\n",
    "                SP_height = list(map(float,SP_data[index+1][5:-1].split(',')))[1:] # Height (cm) (converted to m in dataframe)\n",
    "                SP_h = np.array(SP_height) # Create array of height for conversion to depth \n",
    "                SP_depth = (SP_h-SP_h[-1])*-1 # Depth sets surface as 0\n",
    "                SP_density = list(map(float,SP_data[index+2][5:-1].split(',')))[1:] # Density (kg/m^3)\n",
    "                SP_temperature = list(map(float,SP_data[index+3][5:-1].split(',')))[1:] # Temperature (dec C)\n",
    "                SP_water = list(map(float,SP_data[index+6][5:-1].split(',')))[1:] # Water content (%)\n",
    "                SP_ice = list(map(float,SP_data[index+14][5:-1].split(',')))[1:] # Ice content (%)\n",
    "                SP_air = list(map(float,SP_data[index+15][5:-1].split(',')))[1:] # Air content (%)\n",
    "                t = SP_Timestamp # Grab the correct timestamp\n",
    "                \n",
    "    SP_dict = {'VIR':meta.VIR,'Timestamp':t,'Elevation':SP_Elevation,'Latitude':SP_Latitude,'Longitude':SP_Longitude,'Height':np.array(SP_height)/100,\n",
    "              'Depth':SP_depth/100,'Density':np.array(SP_density),'Temperature':np.array(SP_temperature),\n",
    "              'Ice':np.array(SP_ice)/100,'Air':np.array(SP_air)/100,'Water':np.array(SP_water)/100}\n",
    "    \n",
    "    proF.close()\n",
    "\n",
    "### SNOWPACK smet file ###\n",
    "    \n",
    "    # Get smet info from header\n",
    "    df_smetinfo = pd.read_table(smetFile,skiprows=9,nrows=8,delim_whitespace=True,header=None)\n",
    "    df_smetinfo.reset_index(drop=True,inplace=True)\n",
    "    df_smetinfo.columns = np.array(df_smetinfo.iloc[5])\n",
    "    df_smetinfo = df_smetinfo.drop([2,3,5,6])\n",
    "    df_smetinfo = df_smetinfo.drop(columns=['='])\n",
    "    df_smetinfo.set_index('plot_description',inplace=True)\n",
    "    \n",
    "    # Read in smet file and create arrays of relevant data\n",
    "    df_smet = pd.read_table(smetFile,skiprows=18,delim_whitespace=True,names=np.array(df_smetinfo.iloc[3]))\n",
    "    smetTimestamp = np.array(pd.to_datetime(df_smet.timestamp))\n",
    "    smetSnow = np.array(df_smet.MS_Snow)\n",
    "    smetWind = np.array(df_smet.MS_Wind)\n",
    "    smetRain = np.array(df_smet.MS_Rain)\n",
    "    smetTemp = np.array(df_smet.TA)\n",
    "    smetMelt = np.array(df_smet.MS_melt)\n",
    "    smetEvap = np.array(df_smet.MS_Evap)\n",
    "    smetSubl = np.array(df_smet.MS_Sublimation)\n",
    "    smetRunoff = np.array(df_smet.MS_SN_Runoff)\n",
    "    smetSWE = np.array(df_smet.SWE)\n",
    "    \n",
    "    # Create smet dictionary\n",
    "    smet_dict = {'VIR':meta.VIR,'Latitude':meta.M2_Latitude,'Longitude':meta.M2_Longitude,'Timestamp':smetTimestamp,'Temperature':smetTemp,'Snow':smetSnow,\n",
    "                'Rain':smetRain,'Melt':smetMelt,'Wind':smetWind,'Evaporation':smetEvap,'Sublimation':smetSubl,'Runoff':smetRunoff,'SWE':smetSWE}\n",
    "\n",
    "### CFM ###\n",
    "    \n",
    "    # Get MERRA-2 coordinates from metadata for CFM file name\n",
    "    M2_lat = meta.M2_Latitude\n",
    "    M2_lon = meta.M2_Longitude\n",
    "\n",
    "    # This longitude was manually changed to 0 for SNOWPACK runs, but need to be reverted back to e-13 for CFM\n",
    "    if (M2_lat==-75.0) & (M2_lon==0.0):\n",
    "        M2_lon = -5.920304394294029e-13\n",
    "\n",
    "    # Get CFM file for corresponding coordinate and read it in\n",
    "    CFM_file = pathCFM + 'IDS_baseline_{}_{}_1D_mean'.format(M2_lat,M2_lon) + '/CFMresults.hdf5'\n",
    "    CFM_f = h5.File(CFM_file,'r')\n",
    "\n",
    "    # Extract relevant info from file\n",
    "    CFM_timesteps = CFM_f['density'][1:,0]\n",
    "    CFM_depth = CFM_f['depth'][1:,1:]\n",
    "    CFM_density = CFM_f['density'][1:,1:]\n",
    "    CFM_dip = CFM_f['DIP'][1:,1:]\n",
    "    CFM_f.close()\n",
    "\n",
    "    # Convert decimal years to timestamp\n",
    "    CFM_timestamps = decimal2datetime(CFM_timesteps)\n",
    "\n",
    "    # Find closes CFM timestamp to SUMup timestamp\n",
    "    CFM_closest = min(CFM_timestamps, key=lambda sub: abs(sub - meta.SU_Timestamp))\n",
    "    l = np.where(np.array(CFM_timestamps)==CFM_closest)[0][0]\n",
    "\n",
    "    CFM_dict = {'Latitude':M2_lat,'Longitude':M2_lon,'Timestamp':CFM_timestamps[l],\n",
    "                'Depth':CFM_depth[l],'Density':CFM_density[l],'DIP':CFM_dip[l]}\n",
    "\n",
    "### SUMup ###\n",
    "    \n",
    "    # Extract SUMup dictionary for corresponding SNOWPACK data\n",
    "    SU_DictIndex = df_suMeta[df_suMeta.SU_ID==meta.SU_ID].index[0]\n",
    "    SU_dict = sumup[SU_DictIndex]\n",
    "    \n",
    "    # Create dictionary and append list\n",
    "    dictionaries = {'MERRA-2':smet_dict, 'SNOWPACK':SP_dict,\n",
    "                    'CFM':CFM_dict, 'SUMup':SU_dict}\n",
    "\n",
    "    dict_list.append(dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_list, open(icesheet+'_data.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create interpolated dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(icesheet+'_data.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dictionaries\n",
    "dict_list = []\n",
    "\n",
    "# Regrid models on to observations' vertical scales\n",
    "for i in range(len(data)):\n",
    "\n",
    "    # Select SUMup, CFM, and SNOWPACK keys\n",
    "    SU = data[i]['SUMup']\n",
    "    SP = data[i]['SNOWPACK']\n",
    "    CFM = data[i]['CFM']\n",
    "    \n",
    "    # Get relevant variables from dictionaries\n",
    "    SU_density = SU['Density']\n",
    "    SU_depth = SU['Midpoint']\n",
    "    SP_depth = SP['Depth']\n",
    "    SP_density = SP['Density']\n",
    "    SP_air = SP['Air']\n",
    "    SP_water = SP['Water']\n",
    "    SP_ice = SP['Ice']\n",
    "    CFM_depth = CFM['Depth']\n",
    "    CFM_density = CFM['Density']\n",
    "    CFM_dip = CFM['DIP']\n",
    "\n",
    "    \n",
    "    # Many SUMup measurements are just single point, so just regrid datasets with n > 1\n",
    "    if len(SU_density) > 1:\n",
    "        \n",
    "        # Interpolate modeled density onto observations\n",
    "        SU_densityInt = SU_density\n",
    "        SP_densityInt = griddata(SP_depth,SP_density,SU_depth)\n",
    "        CFM_densityInt = griddata(CFM_depth,CFM_density,SU_depth)\n",
    "        \n",
    "        # Filter out nans\n",
    "        nanfilter = (~np.isnan(SU_densityInt)) & (~np.isnan(SP_densityInt)) & (~np.isnan(CFM_densityInt))\n",
    "        SU_densityFilt = SU_densityInt[nanfilter]\n",
    "        SP_densityFilt = SP_densityInt[nanfilter]\n",
    "        CFM_densityFilt = CFM_densityInt[nanfilter]\n",
    "        \n",
    "        dictionaries = {'SNOWPACK':{'VIR':SP['VIR'],'Timestamp':SP['Timestamp'],\n",
    "                                    'Latitude':SP['Latitude'],'Longitude':SP['Longitude'],\n",
    "                                    'Elevation':SP['Elevation'],'Depth':SU_depth,'Density':SP_densityFilt},\n",
    "                        'CFM':{'Timestamp':CFM['Timestamp'],'Latitude':CFM['Latitude'],'Longitude':CFM['Longitude'],\n",
    "                                    'Elevation':SP['Elevation'],'Depth':SU_depth,'Density':CFM_densityFilt},\n",
    "                        'SUMup':{'CoreID':SU['CoreID'],'Citation':SU['Citation'],'Timestamp':SU['Timestamp'],\n",
    "                                 'Latitude':SU['Latitude'],'Longitude':SU['Longitude'],\n",
    "                                 'Elevation':SU['Elevation'],'Depth':SU_depth,'Density':SU_densityFilt}}\n",
    "    \n",
    "    else:\n",
    "        dictionaries = {'SNOWPACK':{'VIR':SP['VIR'],'Timestamp':SP['Timestamp'],\n",
    "                            'Latitude':SP['Latitude'],'Longitude':SP['Longitude'],\n",
    "                            'Elevation':SP['Elevation'],'Depth':SP_depth,'Density':SP_density},\n",
    "                        'CFM':{'Timestamp':CFM['Timestamp'],'Latitude':CFM['Latitude'],'Longitude':CFM['Longitude'],\n",
    "                                    'Elevation':SP['Elevation'],'Depth':CFM_depth,'Density':CFM_density},\n",
    "                        'SUMup':{'CoreID':SU['CoreID'],'Citation':SU['Citation'],'Timestamp':SU['Timestamp'],\n",
    "                                 'Latitude':SU['Latitude'],'Longitude':SU['Longitude'],\n",
    "                                 'Elevation':SU['Elevation'],'Depth':SU_depth,'Density':SU_density}}\n",
    "\n",
    "    dict_list.append(dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_list, open(icesheet+'_data_interpolated.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpack",
   "language": "python",
   "name": "snowpack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
