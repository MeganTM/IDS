{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionaries for SUMup and SNOWPACK\n",
    "\n",
    "#### Author: Megan Thompson-Munson\n",
    "#### Date created: 20 September 2021\n",
    "\n",
    "This script reads in SUMup observation data and SNOWPACK output, reformats the data, and creates dictionaries that are saved as pickle files.\n",
    "\n",
    "TO DO:\n",
    "- Add CFM capability\n",
    "- Add ```.smet``` capability\n",
    "- Add more comments about input/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN\n",
    "\n",
    "# Select ice sheet\n",
    "icesheet = 'AIS'\n",
    "\n",
    "# Give path of SNOWPACK output data\n",
    "path = '/projects/metm9666/snowpack/Scripts/Spinup/output/'\n",
    "\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from scipy.interpolate import griddata\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in SUMup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumup = pickle.load(open(icesheet+'_SUMup.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMup metadata for finding closest MERRA-2 point\n",
    "suMeta = np.empty((len(sumup),4))\n",
    "\n",
    "for i in range(len(sumup)):\n",
    "    \n",
    "    suData = sumup[i]\n",
    "\n",
    "    suMeta[i,0] = suData['CoreID']\n",
    "    suMeta[i,1] = suData['Timestamp']\n",
    "    suMeta[i,2] = suData['Latitude']\n",
    "    suMeta[i,3] = suData['Longitude']\n",
    "\n",
    "df_suMeta = pd.DataFrame(suMeta,columns=['suID','suTimestamp','suLatitude','suLongitude'])\n",
    "df_suMeta['suTimestamp'] = pd.to_datetime(df_suMeta.suTimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suID</th>\n",
       "      <th>suTimestamp</th>\n",
       "      <th>suLatitude</th>\n",
       "      <th>suLongitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-12-28</td>\n",
       "      <td>-79.446800</td>\n",
       "      <td>-117.963501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-12-12</td>\n",
       "      <td>-79.347900</td>\n",
       "      <td>-116.290497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-12-24</td>\n",
       "      <td>-78.836899</td>\n",
       "      <td>-116.307098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2011-12-14</td>\n",
       "      <td>-78.727997</td>\n",
       "      <td>-114.732201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>-78.424301</td>\n",
       "      <td>-115.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1533.0</td>\n",
       "      <td>2009-02-04</td>\n",
       "      <td>-70.888000</td>\n",
       "      <td>133.285004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1534.0</td>\n",
       "      <td>2009-02-06</td>\n",
       "      <td>-69.827003</td>\n",
       "      <td>134.201996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>1535.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-68.014999</td>\n",
       "      <td>136.464005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>1536.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-67.415001</td>\n",
       "      <td>138.602005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>1537.0</td>\n",
       "      <td>2007-01-26</td>\n",
       "      <td>-71.408302</td>\n",
       "      <td>-9.916700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>894 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       suID suTimestamp  suLatitude  suLongitude\n",
       "0       0.0  2011-12-28  -79.446800  -117.963501\n",
       "1       1.0  2011-12-12  -79.347900  -116.290497\n",
       "2       2.0  2011-12-24  -78.836899  -116.307098\n",
       "3       3.0  2011-12-14  -78.727997  -114.732201\n",
       "4       4.0  2011-12-20  -78.424301  -115.292000\n",
       "..      ...         ...         ...          ...\n",
       "889  1533.0  2009-02-04  -70.888000   133.285004\n",
       "890  1534.0  2009-02-06  -69.827003   134.201996\n",
       "891  1535.0  2009-02-08  -68.014999   136.464005\n",
       "892  1536.0  2009-02-08  -67.415001   138.602005\n",
       "893  1537.0  2007-01-26  -71.408302    -9.916700\n",
       "\n",
       "[894 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_suMeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect SUMup and SNOWPACK metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suID</th>\n",
       "      <th>suTimestamp</th>\n",
       "      <th>suLatitude</th>\n",
       "      <th>suLongitude</th>\n",
       "      <th>spID</th>\n",
       "      <th>spLatitude</th>\n",
       "      <th>spLongitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-12-28</td>\n",
       "      <td>-79.446800</td>\n",
       "      <td>-117.963501</td>\n",
       "      <td>1405.0</td>\n",
       "      <td>-79.5</td>\n",
       "      <td>-118.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-12-12</td>\n",
       "      <td>-79.347900</td>\n",
       "      <td>-116.290497</td>\n",
       "      <td>1406.0</td>\n",
       "      <td>-79.5</td>\n",
       "      <td>-116.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-12-24</td>\n",
       "      <td>-78.836899</td>\n",
       "      <td>-116.307098</td>\n",
       "      <td>1417.0</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>-116.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2011-12-14</td>\n",
       "      <td>-78.727997</td>\n",
       "      <td>-114.732201</td>\n",
       "      <td>1432.0</td>\n",
       "      <td>-78.5</td>\n",
       "      <td>-115.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2011-12-20</td>\n",
       "      <td>-78.424301</td>\n",
       "      <td>-115.292000</td>\n",
       "      <td>1432.0</td>\n",
       "      <td>-78.5</td>\n",
       "      <td>-115.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1533.0</td>\n",
       "      <td>2009-02-04</td>\n",
       "      <td>-70.888000</td>\n",
       "      <td>133.285004</td>\n",
       "      <td>1453.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1534.0</td>\n",
       "      <td>2009-02-06</td>\n",
       "      <td>-69.827003</td>\n",
       "      <td>134.201996</td>\n",
       "      <td>1453.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>1535.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-68.014999</td>\n",
       "      <td>136.464005</td>\n",
       "      <td>1453.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>1536.0</td>\n",
       "      <td>2009-02-08</td>\n",
       "      <td>-67.415001</td>\n",
       "      <td>138.602005</td>\n",
       "      <td>1453.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>153.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>1537.0</td>\n",
       "      <td>2007-01-26</td>\n",
       "      <td>-71.408302</td>\n",
       "      <td>-9.916700</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>-71.5</td>\n",
       "      <td>-10.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>894 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       suID suTimestamp  suLatitude  suLongitude    spID  spLatitude  \\\n",
       "0       0.0  2011-12-28  -79.446800  -117.963501  1405.0       -79.5   \n",
       "1       1.0  2011-12-12  -79.347900  -116.290497  1406.0       -79.5   \n",
       "2       2.0  2011-12-24  -78.836899  -116.307098  1417.0       -79.0   \n",
       "3       3.0  2011-12-14  -78.727997  -114.732201  1432.0       -78.5   \n",
       "4       4.0  2011-12-20  -78.424301  -115.292000  1432.0       -78.5   \n",
       "..      ...         ...         ...          ...     ...         ...   \n",
       "889  1533.0  2009-02-04  -70.888000   133.285004  1453.0       -78.0   \n",
       "890  1534.0  2009-02-06  -69.827003   134.201996  1453.0       -78.0   \n",
       "891  1535.0  2009-02-08  -68.014999   136.464005  1453.0       -78.0   \n",
       "892  1536.0  2009-02-08  -67.415001   138.602005  1453.0       -78.0   \n",
       "893  1537.0  2007-01-26  -71.408302    -9.916700  1562.0       -71.5   \n",
       "\n",
       "     spLongitude  \n",
       "0       -118.125  \n",
       "1       -116.250  \n",
       "2       -116.250  \n",
       "3       -115.000  \n",
       "4       -115.000  \n",
       "..           ...  \n",
       "889      153.125  \n",
       "890      153.125  \n",
       "891      153.125  \n",
       "892      153.125  \n",
       "893      -10.000  \n",
       "\n",
       "[894 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in MERRA-2 location data\n",
    "if icesheet == 'AIS':\n",
    "    df_merra2locs = pd.read_table('AIS_station_list.lst',skiprows=1,delim_whitespace=True,usecols=[0,3,4],names=['Station','Latitude','Longitude'])\n",
    "if icesheet == 'GrIS':\n",
    "    df_merra2locs = pd.read_table('GrIS_station_list.lst',skiprows=1,delim_whitespace=True,usecols=[0,3,4],names=['Station','Latitude','Longitude'])\n",
    "\n",
    "# Extract VIRs\n",
    "VIRs = []\n",
    "for i in range(len(df_merra2locs)):\n",
    "    VIR = df_merra2locs.Station[i][8:]\n",
    "    VIRs.append(VIR)\n",
    "df_merra2locs['VIR'] = VIRs\n",
    "df_merra2locs.drop(columns=['Station'])\n",
    "\n",
    "# Haversine formula for calculating distance between two points on Earth\n",
    "def haversine(lat1,lon1,lat2,lon2):\n",
    "    phi1 = np.deg2rad(lat1)\n",
    "    phi2 = np.deg2rad(lat2)\n",
    "    theta1 = np.deg2rad(lon1)\n",
    "    theta2 = np.deg2rad(lon2)\n",
    "    del_phi = phi2-phi1\n",
    "    del_theta = theta2-theta1\n",
    "    a = np.sin(del_phi/2)**2 + (np.cos(phi1)*np.cos(phi2)*np.sin(del_theta/2)**2)\n",
    "    c = 2*np.arctan2(np.sqrt(a),np.sqrt(1-a))\n",
    "    d = (6371e3)*c # Earth's radius in meters\n",
    "    return d # Meters\n",
    "\n",
    "# Function for finding closest MERRA-2 location to given SUMup location\n",
    "def closest_location(sumuplat,sumuplon):\n",
    "    distance = []\n",
    "    for i in range(len(df_merra2locs)):\n",
    "        lat1 = sumuplat\n",
    "        lon1 = sumuplon\n",
    "        lat2 = df_merra2locs.Latitude[i]\n",
    "        lon2 = df_merra2locs.Longitude[i]\n",
    "        d = haversine(lat1,lon1,lat2,lon2)\n",
    "        distance.append(d)\n",
    "    p = np.where(distance == min(distance))\n",
    "    return df_merra2locs.loc[p]\n",
    "\n",
    "metadata = np.zeros((len(df_suMeta),7))\n",
    "for i in range(len(df_suMeta)):\n",
    "    metadata[i,0] = df_suMeta.suID[i]\n",
    "    metadata[i,1] = np.array(df_suMeta.suTimestamp)[i]\n",
    "    metadata[i,2] = df_suMeta.suLatitude[i]\n",
    "    metadata[i,3] = df_suMeta.suLongitude[i]\n",
    "    merra2 = closest_location(df_suMeta.suLatitude[i],df_suMeta.suLongitude[i])\n",
    "    metadata[i,4] = merra2.VIR.values[0]\n",
    "    metadata[i,5] = merra2.Latitude.values[0]\n",
    "    metadata[i,6] = merra2.Longitude.values[0]\n",
    "\n",
    "# Create dataframe of metadata and turn float date back into timestamp\n",
    "df_meta = pd.DataFrame(metadata,columns=['suID','suTimestamp','suLatitude','suLongitude',\n",
    "                                         'spID','spLatitude','spLongitude'])\n",
    "\n",
    "df_meta['suTimestamp'] = pd.to_datetime(df_meta.suTimestamp)\n",
    "\n",
    "# Ignore anny missing files\n",
    "# if icesheet == 'GrIS':\n",
    "#     df_meta = df_meta[(df_meta.spID!=107)]\n",
    "# if icesheet == 'AIS':\n",
    "#     df_meta = df_meta[(df_meta.spID!=1378)&(df_meta.spID!=1382)&(df_meta.spID!=1395)&(df_meta.spID!=1450)&(df_meta.spID!=1463)&(df_meta.spID!=1567)&(df_meta.spID!=1568)]\n",
    "df_meta.reset_index(drop=True,inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some files don't exist (model errors), so check to see which ones don't and exclude them in the dataframe\n",
    "\n",
    "# Empty list for non-existent files\n",
    "nonfiles = []\n",
    "\n",
    "# Loop through all SNOWPACK output\n",
    "for i in range(len(df_merra2locs)):\n",
    "    \n",
    "    vir = df_merra2locs.VIR.values[i]\n",
    "    \n",
    "    if icesheet == 'GrIS':\n",
    "        file = path+'VIR'+vir+'_GrIS_SUMup.pro'\n",
    "        \n",
    "    if icesheet == 'AIS':\n",
    "        file = path+'VIR'+vir+'_AIS_SUMup.pro'    \n",
    "    \n",
    "    # If the file does not exist, add to lsit\n",
    "    if exists(file) == False:\n",
    "        nonfiles.append(int(vir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1378, 1382, 1395, 1450, 1463, 1567, 1568]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read in SNOWPACK data and create raw dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "\n",
    "for i in range(len(df_meta)):\n",
    "    \n",
    "    meta = df_meta.loc[i]\n",
    "        \n",
    "    if icesheet == 'GrIS':\n",
    "\n",
    "        file = path+'VIR'+str(int(meta.spID))+'_GrIS_SUMup.pro'\n",
    "        smet = path+'VIR'+str(int(meta.spID))+'_GrIS_SUMup.smet'\n",
    "        \n",
    "    if icesheet == 'AIS':\n",
    "        file = path+'VIR'+str(int(meta.spID))+'_AIS_SUMup.pro'\n",
    "        smet = path+'VIR'+str(int(meta.spID))+'_AIS_SUMup.smet'\n",
    "        \n",
    "    \n",
    "### SNOWPACK pro file ###\n",
    "    \n",
    "    # Open *.pro file and read in header (44 lines in length)\n",
    "    f = open(file,'r')\n",
    "    for j in range(44):\n",
    "        header = f.readline()\n",
    "        if j == 1:\n",
    "            VIR = int(header[29:-1])\n",
    "        if j == 2:\n",
    "            spLatitude = float(header[10:-1])\n",
    "        if j == 3:\n",
    "            spLongitude = float(header[11:-1])\n",
    "        if j == 4:\n",
    "            spElevation = float(header[9:-1])\n",
    "    \n",
    "    spTimestamps = [] # Empty list for storing SNOWPACK timestamps\n",
    "\n",
    "    # Read data line by line\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "        linecode = line[0:4] # SNOWPACK gives each data type a 4-digit code\n",
    "\n",
    "        # Extract timestamps and save in a list\n",
    "        if linecode == '0500':\n",
    "            spRawDate = line[5:24]\n",
    "            spDate = datetime.strptime(spRawDate,'%d.%m.%Y %H:%M:%S')\n",
    "            spTimestamp = pd.to_datetime(spDate)\n",
    "            spTimestamps.append(spTimestamp)\n",
    "\n",
    "    # Find SNOWPACK timestamp that's closest to the desired SUMup one\n",
    "    closest = min(spTimestamps, key=lambda sub: abs(sub - meta.suTimestamp))\n",
    "    k = np.where(np.array(spTimestamps)==closest)[0][0]\n",
    "\n",
    "    # Read data and extract lines corresponding to closest timestamp\n",
    "    for line in data:\n",
    "        linecode = line[0:4] # SNOWPACK gives each data type a 4-digit code\n",
    "\n",
    "        if linecode == '0500':\n",
    "            spRawDate = line[5:24]\n",
    "            spDate = datetime.strptime(spRawDate,'%d.%m.%Y %H:%M:%S')\n",
    "            spTimestamp = pd.to_datetime(spDate)\n",
    "\n",
    "            if spTimestamp == closest:\n",
    "\n",
    "                index = k*27 # Each timestamp has 27 elements, so this allows us to get to the start of each new timestamp\n",
    "\n",
    "                # Extract variables of interest by spliting the lines and creating lists of the data\n",
    "                height = list(map(float,data[index+1][5:-1].split(',')))[1:] # Height (cm) (converted to m in dataframe)\n",
    "                h = np.array(height) # Create array of height for conversion to depth \n",
    "                depth = (h-h[-1])*-1 # Depth sets surface as 0\n",
    "                density = list(map(float,data[index+2][5:-1].split(',')))[1:] # Density (kg/m^3)\n",
    "                temperature = list(map(float,data[index+3][5:-1].split(',')))[1:] # Temperature (dec C)\n",
    "                water = list(map(float,data[index+6][5:-1].split(',')))[1:] # Water content (%)\n",
    "                ice = list(map(float,data[index+14][5:-1].split(',')))[1:] # Ice content (%)\n",
    "                air = list(map(float,data[index+15][5:-1].split(',')))[1:] # Air content (%)\n",
    "                t = spTimestamp\n",
    "                \n",
    "    spDict = {'ID':meta.spID,'Timestamp':t,'Elevation':spElevation,'Latitude':spLatitude,'Longitude':spLongitude,'Height':np.array(height)/100,\n",
    "              'Depth':depth/100,'Density':np.array(density),'Temperature':np.array(temperature),\n",
    "              'Ice':np.array(ice)/100,'Air':np.array(air)/100,'Water':np.array(water)/100}\n",
    "    \n",
    "### SNOWPACK smet file ###\n",
    "    \n",
    "    # Get smet info from header\n",
    "    df_smetinfo = pd.read_table(smet,skiprows=9,nrows=8,delim_whitespace=True,header=None)\n",
    "    df_smetinfo.reset_index(drop=True,inplace=True)\n",
    "    df_smetinfo.columns = np.array(df_smetinfo.iloc[5])\n",
    "    df_smetinfo = df_smetinfo.drop([2,3,5,6])\n",
    "    df_smetinfo = df_smetinfo.drop(columns=['='])\n",
    "    df_smetinfo.set_index('plot_description',inplace=True)\n",
    "    \n",
    "    # Read in smet file and create arrays of relevant data\n",
    "    df_smet = pd.read_table(smet,skiprows=18,delim_whitespace=True,names=np.array(df_smetinfo.iloc[3]))\n",
    "    smetTimestamp = np.array(pd.to_datetime(df_smet.timestamp))\n",
    "    smetSnow = np.array(df_smet.MS_Snow)\n",
    "    smetWind = np.array(df_smet.MS_Wind)\n",
    "    smetRain = np.array(df_smet.MS_Rain)\n",
    "    smetTemp = np.array(df_smet.TA)\n",
    "    smetMelt = np.array(df_smet.MS_melt)\n",
    "    smetEvap = np.array(df_smet.MS_Evap)\n",
    "    smetSubl = np.array(df_smet.MS_Sublimation)\n",
    "    smetRunoff = np.array(df_smet.MS_SN_Runoff)\n",
    "    smetSWE = np.array(df_smet.SWE)\n",
    "    \n",
    "    # Create smet dictionary\n",
    "    smetDict = {'ID':meta.spID,'Latitude':meta.spLatitude,'Longitude':meta.spLongitude,'Timestamp':smetTimestamp,'Temperature':smetTemp,'Snow':smetSnow,'Rain':smetRain,\n",
    "                'Melt':smetMelt,'Wind':smetWind,'Evaporation':smetEvap,'Sublimation':smetSubl,'Runoff':smetRunoff,'SWE':smetSWE}\n",
    "    \n",
    "### SUMup ###\n",
    "    \n",
    "    # Extract SUMup dictionary for corresponding SNOWPACK data\n",
    "    suDictIndex = df_suMeta[df_suMeta.suID==meta.suID].index[0]\n",
    "    suDict = sumup[suDictIndex]\n",
    "    \n",
    "    # Create dictionary and append list\n",
    "    dictionaries = {'MERRA-2':smetDict,'SNOWPACK':spDict,'SUMup':suDict}\n",
    "    dict_list.append(dictionaries)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_list, open(icesheet+'_data.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create interpolated dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(icesheet+'_data.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dictionaries\n",
    "dict_list = []\n",
    "\n",
    "# Regrid observations and model output on logarithmic vertical scale\n",
    "for i in range(len(data)):\n",
    "\n",
    "    # Select SUMup and SNOWPACK keys\n",
    "    su = data[i]['SUMup']\n",
    "    sp = data[i]['SNOWPACK']\n",
    "    \n",
    "    # Define relevant variables\n",
    "    sp_depth = sp['Depth']\n",
    "    sp_density = sp['Density']\n",
    "    sp_air = sp['Air']\n",
    "    sp_water = sp['Water']\n",
    "    sp_ice = sp['Ice']\n",
    "    su_density = su['Density']\n",
    "    su_depth = su['Midpoint']\n",
    "    \n",
    "    # Many SUMup measurements are just single point, so just regrid datasets with n > 1\n",
    "    if len(su_density) > 1:\n",
    "        \n",
    "        # Interpolate modeled density onto observations\n",
    "        su_densityInt = su_density\n",
    "        sp_densityInt = griddata(sp_depth,sp_density,su_depth)\n",
    "        \n",
    "        # Filter out nans\n",
    "        nanfilter = (~np.isnan(su_densityInt)) & (~np.isnan(sp_densityInt))\n",
    "        su_densityFilt = su_densityInt[nanfilter]\n",
    "        sp_densityFilt = sp_densityInt[nanfilter]\n",
    "        \n",
    "        dictionaries = {'SNOWPACK':{'ID':sp['ID'],'Timestamp':sp['Timestamp'],\n",
    "                                    'Latitude':sp['Latitude'],'Longitude':sp['Longitude'],\n",
    "                                    'Elevation':sp['Elevation'],'Depth':su_depth,'Density':sp_densityFilt},\n",
    "                        'SUMup':{'CoreID':su['CoreID'],'Citation':su['Citation'],'Timestamp':su['Timestamp'],\n",
    "                                 'Latitude':su['Latitude'],'Longitude':su['Longitude'],\n",
    "                                 'Elevation':su['Elevation'],'Depth':su_depth,'Density':su_densityFilt}}\n",
    "    \n",
    "    else:\n",
    "        dictionaries = {'SNOWPACK':{'ID':sp['ID'],'Timestamp':sp['Timestamp'],\n",
    "                            'Latitude':sp['Latitude'],'Longitude':sp['Longitude'],\n",
    "                            'Elevation':sp['Elevation'],'Depth':sp_depth,'Density':sp_density},\n",
    "                        'SUMup':{'CoreID':su['CoreID'],'Citation':su['Citation'],'Timestamp':su['Timestamp'],\n",
    "                                 'Latitude':su['Latitude'],'Longitude':su['Longitude'],\n",
    "                                 'Elevation':su['Elevation'],'Depth':su_depth,'Density':su_density}}\n",
    "\n",
    "    dict_list.append(dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_list, open(icesheet+'_data_interpolated.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpack",
   "language": "python",
   "name": "snowpack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
